---
phase: 05-advanced-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/rag/project_indexer.py
  - src/rag/dimension_validator.py
  - src/rag/chromadb_client.py
  - src/ui/views/code_editor/ai_panel/rag_context.py
  - src/ui/views/code_editor/ai_panel/chat_panel.py
autonomous: true

must_haves:
  truths:
    - "User can ask AI questions that include relevant codebase context"
    - "RAG retrieves relevant code snippets based on query similarity"
    - "AI responses show sources section with file names and snippets"
    - "Embedding dimension mismatches are caught before write (no corruption)"
  artifacts:
    - path: "src/rag/project_indexer.py"
      provides: "Project-level file indexing for RAG"
      exports: ["ProjectIndexer"]
    - path: "src/rag/dimension_validator.py"
      provides: "Embedding dimension validation"
      exports: ["DimensionValidator", "validate_embeddings"]
    - path: "src/ui/views/code_editor/ai_panel/rag_context.py"
      provides: "RAG context provider for chat panel"
      exports: ["RAGContextProvider"]
  key_links:
    - from: "src/rag/project_indexer.py"
      to: "ChromaDBClient"
      via: "add_chunks with validated embeddings"
      pattern: "chromadb\\.add_chunks"
    - from: "src/rag/chromadb_client.py"
      to: "DimensionValidator"
      via: "validation before write"
      pattern: "validate_before_write"
    - from: "src/ui/views/code_editor/ai_panel/chat_panel.py"
      to: "RAGContextProvider"
      via: "get_context call before AI request"
      pattern: "rag_context\\.get_context"
---

<objective>
Implement project-level RAG for codebase context with embedding dimension validation to prevent corruption.

Purpose: Users get AI responses informed by their actual codebase, making the assistant more helpful for project-specific questions. Dimension validation prevents the ChromaDB corruption that occurs when embedding models change.

Output: ProjectIndexer for code file indexing, DimensionValidator for write protection, RAGContextProvider for chat integration, Sources display in AI responses.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-advanced-integration/05-RESEARCH.md
@.planning/phases/05-advanced-integration/05-CONTEXT.md

# Key existing files
@src/rag/chromadb_client.py
@src/rag/embeddings.py
@src/rag/models.py
@src/ui/views/code_editor/ai_panel/chat_panel.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DimensionValidator for embedding consistency</name>
  <files>
    src/rag/dimension_validator.py
    src/rag/chromadb_client.py
  </files>
  <action>
Create DimensionValidator class (src/rag/dimension_validator.py):

1. Define EXPECTED_DIMENSIONS dict:
   ```python
   EXPECTED_DIMENSIONS = {
       "all-MiniLM-L6-v2": 384,
       "text-embedding-ada-002": 1536,
       "text-embedding-3-small": 1536,
   }
   ```

2. Implement validate_before_write(collection_id, embeddings, chromadb_client, model_name) -> None:
   - Check embeddings list is not empty
   - Verify all embeddings have same dimension (len check)
   - If model_name in EXPECTED_DIMENSIONS, verify dimension matches
   - If collection exists, verify new embeddings match existing dimension
   - Raise ValueError with clear message on mismatch

3. Add get_collection_dimension(chromadb_client, collection_id) -> int | None:
   - Query collection metadata for stored dimension
   - Return None if collection doesn't exist

Integrate into ChromaDBClient:

4. Update add_chunks method to validate before write:
   ```python
   async def add_chunks(self, collection_id, chunks, embeddings, model_name="all-MiniLM-L6-v2"):
       validator = DimensionValidator()
       validator.validate_before_write(collection_id, embeddings, self, model_name)
       # ... existing add logic
   ```

5. Store model_name in collection metadata on create:
   ```python
   metadata={"embedding_dim": dim, "model_name": model_name}
   ```
  </action>
  <verify>
    pytest tests/unit/test_dimension_validator.py -v (create test)
    Test: Mismatched dimensions raise ValueError with clear message
    Test: Consistent dimensions pass validation
  </verify>
  <done>
    DimensionValidator.validate_before_write raises on dimension mismatch
    ChromaDBClient.add_chunks validates embeddings before write
    Collection metadata includes model_name for future validation
    INTG-04 requirement satisfied
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ProjectIndexer for codebase file indexing</name>
  <files>
    src/rag/project_indexer.py
  </files>
  <action>
Create ProjectIndexer class:

1. Define SUPPORTED_EXTENSIONS set:
   ```python
   SUPPORTED_EXTENSIONS = {
       '.py', '.js', '.ts', '.jsx', '.tsx', '.mjs',
       '.md', '.txt', '.json', '.yaml', '.yml',
       '.html', '.css', '.scss', '.sql', '.sh', '.bash',
       '.java', '.kt', '.go', '.rs', '.c', '.cpp', '.h',
   }
   ```

2. Implement __init__(chromadb_client, embedding_manager):
   - Store references to dependencies
   - Initialize _file_hashes: dict[str, str] for incremental indexing
   - Set chunk_size = 500 (characters) and chunk_overlap = 50

3. Implement async index_project(project_root: str) -> dict:
   - Generate collection_id = f"project-{hash_path(project_root)}"
   - Create collection if not exists
   - Iterate files with supported extensions (use pathlib)
   - Skip hidden files/folders (starting with .)
   - Skip files > 50KB (warn via logging)
   - For each file: compute hash, skip if unchanged, else index
   - Return stats: {"indexed": N, "skipped": M, "errors": E}

4. Implement _index_file(collection_id, file_path):
   - Read file content
   - Split into chunks with overlap
   - Generate embeddings via embedding_manager.embed_batch()
   - Create Chunk objects with metadata: source_path, language, chunk_index
   - Call chromadb.add_chunks() (will validate dimensions)

5. Implement async query_context(query, project_root, top_k=5) -> list[dict]:
   - Generate query embedding
   - Query ChromaDB collection
   - Return list of {content, source_path, similarity}

6. Implement _compute_hash(file_path) -> str:
   - Use hashlib.md5 on file content for change detection
  </action>
  <verify>
    pytest tests/unit/test_project_indexer.py -v (create test with mock files)
    Test: Index project returns correct stats
    Test: Unchanged files are skipped (incremental)
    Test: query_context returns relevant chunks
  </verify>
  <done>
    ProjectIndexer.index_project indexes all supported files
    Incremental indexing skips unchanged files (hash-based)
    query_context returns relevant code snippets for query
    Large files (>50KB) are skipped with warning
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate RAG context into ChatPanel with Sources display</name>
  <files>
    src/ui/views/code_editor/ai_panel/rag_context.py
    src/ui/views/code_editor/ai_panel/chat_panel.py
    src/ui/views/code_editor/ai_panel/__init__.py
  </files>
  <action>
Create RAGContextProvider (src/ui/views/code_editor/ai_panel/rag_context.py):

1. Implement __init__(chromadb_client, embedding_manager):
   - Store dependencies
   - Create ProjectIndexer instance
   - Track _indexed_projects: set[str] for deferred indexing

2. Implement async ensure_indexed(project_root: str):
   - If project_root not in _indexed_projects, run index_project()
   - Add to set after successful index
   - This is called lazily on first query

3. Implement async get_context(query: str, project_root: str | None) -> tuple[str, list[dict]]:
   - If no project_root, return ("", [])
   - Ensure project is indexed
   - Query for top 5 relevant chunks
   - Format as context string: "## Relevant Code\n\n### {source_path}\n```\n{content}\n```"
   - Return (context_string, sources_list)

Integrate into ChatPanel:

4. In chat_panel.py, add rag_provider: RAGContextProvider | None parameter to __init__
5. Before sending message to AI:
   - Get project_root from editor state (if available)
   - Call rag_provider.get_context(user_message, project_root)
   - Prepend context to system prompt: f"Use this codebase context:\n{context}\n\n"
   - Store sources for display

6. Add collapsible Sources section in message display:
   - After AI response, show "Sources (N files)" expandable
   - When expanded, list source files with snippets
   - Use ft.ExpansionTile for collapse/expand

7. Update __init__.py exports to include RAGContextProvider
  </action>
  <verify>
    pytest tests/unit/test_rag_context.py -v (create test)
    Manual: Open project folder, ask question in chat, verify sources shown
    Manual: Sources section collapses/expands correctly
  </verify>
  <done>
    RAGContextProvider.get_context returns relevant code snippets
    ChatPanel includes RAG context in AI requests automatically
    Sources section displays below AI responses (collapsible)
    First query triggers background indexing (lazy initialization)
    INTG-02 requirement satisfied
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. pytest tests/unit/test_dimension_validator.py tests/unit/test_project_indexer.py tests/unit/test_rag_context.py -v
2. Open code editor, open a project folder with Python files
3. Ask AI "What does the main function do?" - verify it uses codebase context
4. Check Sources section shows relevant files
5. Try adding embeddings with wrong dimension to ChromaDB - verify error
</verification>

<success_criteria>
- RAG retrieves relevant code snippets based on semantic similarity
- AI responses are informed by actual project codebase
- Sources section shows which files were used for context
- Dimension mismatches caught before write with clear error message
- INTG-02 and INTG-04 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/05-advanced-integration/05-02-SUMMARY.md`
</output>
