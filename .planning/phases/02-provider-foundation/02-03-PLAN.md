---
phase: 02-provider-foundation
plan: 03
type: execute
wave: 2
depends_on: []
files_modified:
  - src/ai/providers/ollama.py
  - src/ui/views/ai_hub/state.py
  - src/ui/views/ai_hub/providers.py
autonomous: true

must_haves:
  truths:
    - "User can see Ollama connection status (connected/disconnected) in the UI"
    - "User sees available Ollama models refresh automatically when connected"
    - "User can manually trigger a model refresh with a button"
    - "User sees clear error when Ollama is not running"
  artifacts:
    - path: "src/ai/providers/ollama.py"
      provides: "Enhanced Ollama with status checking"
      exports: ["OllamaProvider"]
      contains: "check_status"
    - path: "src/ui/views/ai_hub/state.py"
      provides: "Ollama status in AIHubState"
      contains: "ollama_last_refresh"
    - path: "src/ui/views/ai_hub/providers.py"
      provides: "Ollama status indicator and refresh button"
      min_lines: 50
  key_links:
    - from: "src/ui/views/ai_hub/providers.py"
      to: "src/ai/providers/ollama.py"
      via: "status check call"
      pattern: "check_status|check_ollama"
    - from: "src/ui/views/ai_hub/providers.py"
      to: "src/ui/views/ai_hub/state.py"
      via: "state update"
      pattern: "set_ollama_status"
---

<objective>
Enhance Ollama provider with better service discovery and status UI.

Purpose: Give users clear visibility into Ollama connection status and available models, with auto-refresh on connect and manual refresh capability.

Output: Enhanced OllamaProvider with status checking, AIHubState with Ollama status tracking, and UI components for status display and refresh.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-provider-foundation/02-RESEARCH.md
@.planning/phases/02-provider-foundation/02-CONTEXT.md
@.planning/phases/01-stability-audit/01-04-SUMMARY.md

# Reference implementations
@src/ai/providers/ollama.py
@src/ui/views/ai_hub/state.py
@src/ui/views/ai_hub/providers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add status checking to OllamaProvider</name>
  <files>src/ai/providers/ollama.py</files>
  <action>
Add a dedicated status checking method to OllamaProvider that can be called independently of initialization:

**Add new method:**
```python
async def check_status(self) -> tuple[bool, list[str], str | None]:
    """
    Check Ollama connection status and available models.

    Returns:
        Tuple of (is_connected, model_names, error_message)
        - is_connected: True if Ollama is running and responsive
        - model_names: List of available model names (empty if not connected)
        - error_message: Human-readable error if not connected, None otherwise
    """
```

**Implementation details:**
1. Use httpx GET to `/api/tags` with 5-second timeout
2. On success: Return (True, [model names], None)
3. On connection refused: Return (False, [], "Ollama is not running. Start with: ollama serve")
4. On timeout: Return (False, [], "Ollama is slow to respond. Check if models are loading.")
5. On other error: Return (False, [], f"Could not connect to Ollama: {error}")

**PITFALL - First request slow:**
Ollama lazy-loads models. Consider adding optional "warm up" call with empty prompt to preload the first model after connection confirmed.

**Also add refresh method:**
```python
async def refresh_models(self) -> list[str]:
    """Force refresh of available models list. Returns model names."""
```

This calls check_status internally and updates _available_models.
  </action>
  <verify>
```python
import asyncio
from src.ai.providers.ollama import OllamaProvider

async def test():
    p = OllamaProvider()
    connected, models, error = await p.check_status()
    print(f"Connected: {connected}, Models: {models}, Error: {error}")

asyncio.run(test())
```
If Ollama is running, should show models. If not, should show "Ollama is not running" error.
  </verify>
  <done>OllamaProvider has check_status() and refresh_models() methods with proper error messages</done>
</task>

<task type="auto">
  <name>Task 2: Extend AIHubState for Ollama status</name>
  <files>src/ui/views/ai_hub/state.py</files>
  <action>
Extend the existing AIHubState dataclass to track more Ollama status information:

**Add new fields:**
```python
@dataclass
class AIHubState:
    # ... existing fields ...

    # Enhanced Ollama status
    ollama_connected: bool = False  # Already exists
    ollama_models: list[str] = field(default_factory=list)  # Already exists
    ollama_error: str | None = None  # NEW: Error message if not connected
    ollama_last_refresh: float | None = None  # NEW: Timestamp of last successful refresh
    ollama_refreshing: bool = False  # NEW: True during refresh operation
```

**Update set_ollama_status method:**
```python
def set_ollama_status(
    self,
    connected: bool,
    models: list[str] | None = None,
    error: str | None = None,
) -> None:
    """Update Ollama connection status."""
    self.ollama_connected = connected
    if models is not None:
        self.ollama_models = models
    self.ollama_error = error
    if connected:
        self.ollama_last_refresh = time.time()
    self.notify()
```

**Add new methods:**
```python
def set_ollama_refreshing(self, refreshing: bool) -> None:
    """Set refreshing state (for loading indicator)."""
    self.ollama_refreshing = refreshing
    self.notify()
```

Import `time` at top of file if not already imported.
  </action>
  <verify>
```python
from src.ui.views.ai_hub.state import AIHubState
import time

state = AIHubState()
state.set_ollama_status(True, ["llama3", "codellama"], None)
print(f"Connected: {state.ollama_connected}, Models: {state.ollama_models}")
print(f"Last refresh: {state.ollama_last_refresh}")
```
  </verify>
  <done>AIHubState extended with ollama_error, ollama_last_refresh, ollama_refreshing fields and updated methods</done>
</task>

<task type="auto">
  <name>Task 3: Add Ollama status UI to providers tab</name>
  <files>src/ui/views/ai_hub/providers.py</files>
  <action>
Add Ollama status indicator and refresh button to the providers tab.

**Find the Ollama provider card section and enhance it:**

1. **Status indicator:** Show connection status with icon
   - Connected (green): "Connected" with check icon
   - Disconnected (red): Error message with warning icon
   - Refreshing (blue): "Refreshing..." with spinner

2. **Model count:** Show "N models available" when connected

3. **Refresh button:** IconButton with refresh icon
   - On click: Call async refresh, update state
   - Disabled while refreshing
   - Show loading state during refresh

**Implementation pattern (using Flet):**
```python
def _build_ollama_status(self) -> ft.Control:
    """Build Ollama status indicator with refresh button."""
    if self.state.ollama_refreshing:
        status_icon = ft.ProgressRing(width=16, height=16)
        status_text = "Refreshing..."
        status_color = ft.Colors.BLUE
    elif self.state.ollama_connected:
        status_icon = ft.Icon(ft.Icons.CHECK_CIRCLE, color=ft.Colors.GREEN, size=16)
        status_text = f"Connected ({len(self.state.ollama_models)} models)"
        status_color = ft.Colors.GREEN
    else:
        status_icon = ft.Icon(ft.Icons.WARNING, color=ft.Colors.RED, size=16)
        status_text = self.state.ollama_error or "Not connected"
        status_color = ft.Colors.RED

    refresh_btn = ft.IconButton(
        icon=ft.Icons.REFRESH,
        tooltip="Refresh models",
        on_click=self._on_refresh_ollama,
        disabled=self.state.ollama_refreshing,
    )

    return ft.Row([
        status_icon,
        ft.Text(status_text, color=status_color),
        refresh_btn,
    ])
```

**Add async handler:**
```python
async def _on_refresh_ollama(self, e):
    self.state.set_ollama_refreshing(True)
    self.page.update()

    from src.ai.providers.ollama import OllamaProvider
    provider = OllamaProvider()
    connected, models, error = await provider.check_status()

    self.state.set_ollama_refreshing(False)
    self.state.set_ollama_status(connected, models, error)
    self.page.update()
```

**Auto-refresh on mount:** If the providers tab component has a did_mount or similar lifecycle hook, call the refresh automatically on first load.
  </action>
  <verify>
Manual verification: Run the app and navigate to AI Hub -> Providers tab.
- With Ollama running: Should show green "Connected (N models)" with refresh button
- With Ollama stopped: Should show red "Ollama is not running..." with refresh button
- Click refresh: Should show "Refreshing..." briefly then update status
  </verify>
  <done>Providers tab shows Ollama connection status with model count and refresh button</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Status check works:**
   ```bash
   python -c "
   import asyncio
   from src.ai.providers.ollama import OllamaProvider
   async def test():
       p = OllamaProvider()
       connected, models, error = await p.check_status()
       print(f'Connected: {connected}')
       print(f'Models: {models}')
       print(f'Error: {error}')
   asyncio.run(test())
   "
   ```

2. **State tracks Ollama status:**
   ```bash
   python -c "
   from src.ui.views.ai_hub.state import AIHubState
   s = AIHubState()
   s.set_ollama_status(True, ['model1'], None)
   print(f'Has last_refresh: {s.ollama_last_refresh is not None}')
   "
   ```

3. **All existing tests pass:**
   ```bash
   pytest tests/unit/test_ai_hub_refactor.py -v
   ```

4. **No ruff errors:**
   ```bash
   ruff check src/ai/providers/ollama.py src/ui/views/ai_hub/state.py src/ui/views/ai_hub/providers.py
   ```
</verification>

<success_criteria>
- OllamaProvider.check_status() returns (connected, models, error) tuple
- OllamaProvider.refresh_models() forces model list refresh
- AIHubState tracks ollama_error, ollama_last_refresh, ollama_refreshing
- Providers tab shows Ollama status indicator with connection state
- Providers tab has working refresh button that updates status
- Error messages are user-friendly ("Ollama is not running" not "Connection refused")
- All existing tests pass
- No ruff/lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-provider-foundation/02-03-SUMMARY.md`
</output>
