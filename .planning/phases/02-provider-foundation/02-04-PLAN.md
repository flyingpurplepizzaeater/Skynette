---
phase: 02-provider-foundation
plan: 04
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - src/ai/gateway.py
  - src/ai/providers/base.py
  - src/ai/providers/gemini.py
  - src/ai/providers/grok.py
  - src/ai/providers/anthropic.py
  - src/ai/providers/openai.py
autonomous: true

must_haves:
  truths:
    - "RateLimitInfo dataclass calculates usage percentage and approaching-limit flag"
    - "User receives clear error with retry timing when rate limited"
    - "User sees partial content preserved when streaming fails mid-response"
    - "User sees '[Response interrupted]' marker when stream fails"
  artifacts:
    - path: "src/ai/gateway.py"
      provides: "RateLimitInfo dataclass and streaming recovery"
      contains: "RateLimitInfo"
      exports: ["RateLimitInfo", "AIStreamChunk", "StreamInterruptedError"]
    - path: "src/ai/providers/base.py"
      provides: "Streaming recovery wrapper method"
      contains: "_stream_with_recovery"
  key_links:
    - from: "src/ai/providers/gemini.py"
      to: "_stream_with_recovery"
      via: "base class method"
      pattern: "_stream_with_recovery"
    - from: "src/ai/gateway.py"
      to: "RateLimitInfo"
      via: "dataclass export"
      pattern: "class RateLimitInfo"
---

<objective>
Implement rate limit handling with pre-emptive throttling and streaming failure recovery.

Purpose: Users should never be surprised by rate limit errors or lose content when streams fail. They get clear error messages with retry timing and partial content is preserved when streams fail mid-response.

Output: Rate limit tracking infrastructure, streaming failure recovery wrapper, and enhanced error reporting across all cloud providers.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-provider-foundation/02-RESEARCH.md
@.planning/phases/02-provider-foundation/02-CONTEXT.md

# Reference implementations
@src/ai/gateway.py
@src/ai/providers/base.py
@src/ai/providers/anthropic.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add rate limit and streaming error dataclasses</name>
  <files>src/ai/gateway.py</files>
  <action>
Add dataclasses for rate limit tracking and enhanced streaming chunks:

**Add RateLimitInfo dataclass:**
```python
@dataclass
class RateLimitInfo:
    """Rate limit information from provider response headers."""
    limit_requests: int = 0  # Max requests per window
    remaining_requests: int = 0  # Requests remaining
    reset_time: float | None = None  # Unix timestamp when limit resets
    limit_tokens: int = 0  # Max tokens per window (if applicable)
    remaining_tokens: int = 0  # Tokens remaining

    @property
    def usage_percentage(self) -> float:
        """Calculate usage percentage (0.0 to 1.0)."""
        if self.limit_requests == 0:
            return 0.0
        return 1.0 - (self.remaining_requests / self.limit_requests)

    @property
    def is_approaching_limit(self) -> bool:
        """True if usage is at 80% or above."""
        return self.usage_percentage >= 0.8

    @property
    def seconds_until_reset(self) -> float | None:
        """Seconds until rate limit resets, or None if unknown."""
        if self.reset_time is None:
            return None
        import time
        return max(0, self.reset_time - time.time())
```

**Enhance AIStreamChunk:**
Update the existing AIStreamChunk dataclass to include error info:
```python
@dataclass
class AIStreamChunk:
    """A chunk from a streaming response."""
    content: str
    is_final: bool = False
    usage: Optional[dict] = None
    error: Optional[dict] = None  # NEW: Error info if stream interrupted
    rate_limit: Optional[RateLimitInfo] = None  # NEW: Rate limit info from response
```

**Add StreamInterruptedError exception class:**
```python
class StreamInterruptedError(Exception):
    """Raised when a stream is interrupted mid-response."""
    def __init__(self, partial_content: str, cause: Exception):
        self.partial_content = partial_content
        self.cause = cause
        super().__init__(f"Stream interrupted after {len(partial_content)} chars: {cause}")
```

Export all new classes in module.
  </action>
  <verify>
```python
from src.ai.gateway import RateLimitInfo, AIStreamChunk, StreamInterruptedError

# Test RateLimitInfo
info = RateLimitInfo(limit_requests=100, remaining_requests=15)
print(f"Usage: {info.usage_percentage:.0%}")  # Should print 85%
print(f"Approaching limit: {info.is_approaching_limit}")  # Should print True

# Test enhanced AIStreamChunk
chunk = AIStreamChunk(content="test", error={"type": "timeout"})
print(f"Has error: {chunk.error is not None}")  # Should print True
```
  </verify>
  <done>RateLimitInfo dataclass, enhanced AIStreamChunk, and StreamInterruptedError added to gateway.py</done>
</task>

<task type="auto">
  <name>Task 2: Add streaming recovery wrapper to BaseProvider</name>
  <files>src/ai/providers/base.py</files>
  <action>
Add a streaming recovery wrapper method to BaseProvider that all providers can use:

**Import updates:**
```python
from src.ai.gateway import (
    AICapability,
    AIMessage,
    AIResponse,
    AIStreamChunk,
    GenerationConfig,
    StreamInterruptedError,  # NEW
)
```

**Add wrapper method:**
```python
async def _stream_with_recovery(
    self,
    stream_generator: AsyncIterator[AIStreamChunk],
    interrupt_marker: str = "\n\n[Response interrupted]",
) -> AsyncIterator[AIStreamChunk]:
    """
    Wrap a stream generator with partial response preservation.

    If the stream fails mid-response, yields a final chunk with:
    - The interrupt marker appended to content
    - Error info in the error field
    - is_final=True

    Usage in subclass:
        async for chunk in self._stream_with_recovery(self._raw_stream(messages, config)):
            yield chunk
    """
    buffer = []
    try:
        async for chunk in stream_generator:
            buffer.append(chunk.content)
            yield chunk
    except Exception as e:
        partial_content = "".join(buffer)
        # Yield final chunk with error info
        yield AIStreamChunk(
            content=interrupt_marker,
            is_final=True,
            error={
                "type": type(e).__name__,
                "message": str(e),
                "partial_content_length": len(partial_content),
            }
        )
        # Re-raise wrapped exception for caller handling
        raise StreamInterruptedError(partial_content=partial_content, cause=e)
```

**Add rate limit header parsing helper:**
```python
def _parse_rate_limit_headers(self, headers: dict) -> "RateLimitInfo":
    """
    Parse rate limit info from response headers.

    Different providers use different header names. This method handles
    common patterns. Subclasses can override for provider-specific parsing.
    """
    from src.ai.gateway import RateLimitInfo

    # Common header patterns
    return RateLimitInfo(
        limit_requests=int(headers.get("x-ratelimit-limit-requests", 0)),
        remaining_requests=int(headers.get("x-ratelimit-remaining-requests", 0)),
        limit_tokens=int(headers.get("x-ratelimit-limit-tokens", 0)),
        remaining_tokens=int(headers.get("x-ratelimit-remaining-tokens", 0)),
        # Parse reset time if present (Retry-After header)
        reset_time=self._parse_reset_time(headers.get("retry-after")),
    )

def _parse_reset_time(self, retry_after: str | None) -> float | None:
    """Parse Retry-After header to Unix timestamp."""
    if retry_after is None:
        return None
    import time
    try:
        # Try as seconds
        return time.time() + int(retry_after)
    except ValueError:
        # Try as HTTP date (not common, skip for now)
        return None
```
  </action>
  <verify>
```python
import asyncio
from src.ai.providers.base import BaseProvider
from src.ai.gateway import AIStreamChunk

# Test that methods exist
print(hasattr(BaseProvider, '_stream_with_recovery'))
print(hasattr(BaseProvider, '_parse_rate_limit_headers'))
```
  </verify>
  <done>BaseProvider has _stream_with_recovery() and _parse_rate_limit_headers() helper methods</done>
</task>

<task type="auto">
  <name>Task 3: Apply streaming recovery to cloud providers</name>
  <files>src/ai/providers/gemini.py, src/ai/providers/grok.py, src/ai/providers/anthropic.py, src/ai/providers/openai.py</files>
  <action>
Update cloud providers to use the streaming recovery wrapper.

**For each provider (gemini.py, grok.py, anthropic.py, openai.py):**

1. Rename current chat_stream to _raw_chat_stream
2. Create new chat_stream that wraps with recovery:

```python
async def chat_stream(
    self,
    messages: list[AIMessage],
    config: GenerationConfig,
) -> AsyncIterator[AIStreamChunk]:
    """Stream a chat response with error recovery."""
    try:
        async for chunk in self._stream_with_recovery(
            self._raw_chat_stream(messages, config)
        ):
            yield chunk
    except StreamInterruptedError:
        # Error already yielded as final chunk, just return
        return

async def _raw_chat_stream(
    self,
    messages: list[AIMessage],
    config: GenerationConfig,
) -> AsyncIterator[AIStreamChunk]:
    """Internal streaming implementation."""
    # ... existing streaming code moves here ...
```

**For Anthropic specifically:** Also add rate limit tracking from response headers after stream completes (Anthropic returns headers in final message).

**Import StreamInterruptedError** at top of each file:
```python
from src.ai.gateway import (
    # ... existing imports ...
    StreamInterruptedError,
)
```

**CONTEXT DECISION (from 02-CONTEXT.md):**
- Keep partial content when streaming fails
- Show "[Response interrupted]" marker (handled by wrapper)
- No auto-retry - show error with manual Retry button (UI handles this in future phase)
  </action>
  <verify>
For each provider, verify the structure:
```python
from src.ai.providers.anthropic import AnthropicProvider
from src.ai.providers.openai import OpenAIProvider

# Check methods exist
print(hasattr(AnthropicProvider, 'chat_stream'))
print(hasattr(AnthropicProvider, '_raw_chat_stream'))
print(hasattr(OpenAIProvider, 'chat_stream'))
print(hasattr(OpenAIProvider, '_raw_chat_stream'))
```

Run existing tests to ensure no regression:
```bash
pytest tests/unit/test_ai_providers.py -v
```
  </verify>
  <done>All cloud providers use streaming recovery wrapper, preserving partial content on failure</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **RateLimitInfo works:**
   ```bash
   python -c "
   from src.ai.gateway import RateLimitInfo
   info = RateLimitInfo(limit_requests=100, remaining_requests=15)
   print(f'Usage: {info.usage_percentage:.0%}')
   print(f'Approaching: {info.is_approaching_limit}')
   "
   ```
   Expected: Usage: 85%, Approaching: True

2. **StreamInterruptedError works:**
   ```bash
   python -c "
   from src.ai.gateway import StreamInterruptedError
   e = StreamInterruptedError('partial text', ValueError('test'))
   print(f'Partial length: {len(e.partial_content)}')
   "
   ```
   Expected: Partial length: 12

3. **Providers have recovery wrapper:**
   ```bash
   python -c "
   from src.ai.providers.anthropic import AnthropicProvider
   print('_raw_chat_stream' in dir(AnthropicProvider))
   "
   ```
   Expected: True

4. **All existing tests pass:**
   ```bash
   pytest tests/unit/test_ai_providers.py tests/unit/test_ai_hub_refactor.py -v
   ```

5. **No ruff errors:**
   ```bash
   ruff check src/ai/gateway.py src/ai/providers/base.py src/ai/providers/gemini.py src/ai/providers/grok.py src/ai/providers/anthropic.py src/ai/providers/openai.py
   ```
</verification>

<success_criteria>
- RateLimitInfo dataclass exists with usage_percentage, is_approaching_limit, seconds_until_reset
- AIStreamChunk has error and rate_limit optional fields
- StreamInterruptedError exception class exists with partial_content and cause
- BaseProvider has _stream_with_recovery() and _parse_rate_limit_headers() methods
- All cloud providers (Gemini, Grok, Anthropic, OpenAI) wrap streaming with recovery
- Streaming failures preserve partial content and add "[Response interrupted]" marker
- All existing tests pass
- No ruff/lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-provider-foundation/02-04-SUMMARY.md`
</output>
