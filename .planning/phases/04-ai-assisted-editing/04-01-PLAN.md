---
phase: 04-ai-assisted-editing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ui/views/code_editor/ai_panel/__init__.py
  - src/ui/views/code_editor/ai_panel/chat_state.py
  - src/ai/completions/__init__.py
  - src/ai/completions/token_counter.py
  - src/services/diff/__init__.py
  - src/services/diff/diff_service.py
  - src/services/diff/models.py
autonomous: true

must_haves:
  truths:
    - "ChatState can store messages and notify listeners on change"
    - "TokenCounter can estimate token counts for code context"
    - "DiffService can generate unified diff hunks from two strings"
  artifacts:
    - path: "src/ui/views/code_editor/ai_panel/chat_state.py"
      provides: "ChatState and ChatMessage dataclasses"
      contains: "class ChatState"
    - path: "src/ai/completions/token_counter.py"
      provides: "Token counting with tiktoken"
      contains: "class TokenCounter"
    - path: "src/services/diff/diff_service.py"
      provides: "Diff generation using difflib"
      contains: "class DiffService"
  key_links:
    - from: "chat_state.py"
      to: "EditorState pattern"
      via: "listener/notify pattern"
      pattern: "add_listener.*notify"
---

<objective>
Create foundation services for AI-assisted editing: ChatState for chat panel state management, TokenCounter for context size estimation, and DiffService for change preview.

Purpose: These services provide the data layer that UI components (Plans 02-04) will consume. Without them, the UI has no state to manage or services to call.

Output: Three service modules ready for UI integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-assisted-editing/04-CONTEXT.md
@.planning/phases/04-ai-assisted-editing/04-RESEARCH.md

# Existing patterns to follow
@src/ui/views/code_editor/state.py (EditorState listener/notify pattern)
@src/ai/gateway.py (AIMessage dataclass pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ChatState and ChatMessage models</name>
  <files>
    src/ui/views/code_editor/ai_panel/__init__.py
    src/ui/views/code_editor/ai_panel/chat_state.py
  </files>
  <action>
    Create ChatState following the EditorState listener/notify pattern:

    1. Create `src/ui/views/code_editor/ai_panel/__init__.py` with exports
    2. Create `src/ui/views/code_editor/ai_panel/chat_state.py` with:

    ```python
    @dataclass
    class ChatMessage:
        role: str  # "user", "assistant", "system"
        content: str
        timestamp: float = field(default_factory=time.time)
        code_context: str | None = None  # Attached code snippet

    @dataclass
    class ChatState:
        messages: list[ChatMessage] = field(default_factory=list)
        is_streaming: bool = False
        selected_provider: str | None = None
        context_mode: str = "current_file"  # current_file, imports, project

        _listeners: list[Callable[[], None]] = field(default_factory=list, repr=False)

        def add_listener(self, callback: Callable[[], None]) -> None
        def remove_listener(self, callback: Callable[[], None]) -> None
        def notify(self) -> None

        def add_message(self, role: str, content: str, code_context: str | None = None) -> None
        def update_last_message(self, content: str) -> None  # For streaming
        def clear_messages(self) -> None
        def set_streaming(self, is_streaming: bool) -> None
        def set_provider(self, provider: str) -> None
        def dispose(self) -> None
    ```

    Follow EditorState exactly for listener pattern. Include docstrings.
  </action>
  <verify>
    ```bash
    python -c "from src.ui.views.code_editor.ai_panel import ChatState, ChatMessage; s = ChatState(); s.add_message('user', 'test'); print(f'Messages: {len(s.messages)}')"
    ```
    Should print "Messages: 1"
  </verify>
  <done>ChatState stores messages and notifies listeners on add_message/update_last_message/clear calls</done>
</task>

<task type="auto">
  <name>Task 2: Create TokenCounter service</name>
  <files>
    src/ai/completions/__init__.py
    src/ai/completions/token_counter.py
  </files>
  <action>
    Create TokenCounter for estimating context token usage:

    1. Install tiktoken: `pip install tiktoken` (add to requirements.txt if exists)
    2. Create `src/ai/completions/__init__.py` with exports
    3. Create `src/ai/completions/token_counter.py`:

    ```python
    import tiktoken

    class TokenCounter:
        """Estimate token counts for different providers."""

        def __init__(self):
            # cl100k_base for GPT-4/3.5
            self._openai_enc = tiktoken.get_encoding("cl100k_base")
            # p50k_base as fallback approximation
            self._fallback_enc = tiktoken.get_encoding("p50k_base")

        def count_tokens(self, text: str, provider: str = "openai") -> int:
            """Count tokens for the given text and provider.

            Args:
                text: Text to count tokens for.
                provider: Provider name (openai, anthropic, gemini, grok, ollama).

            Returns:
                Estimated token count.
            """
            if provider in ("openai", "gpt-4", "gpt-3.5"):
                return len(self._openai_enc.encode(text))
            # Other providers use fallback (approximation)
            return len(self._fallback_enc.encode(text))

        def estimate_heuristic(self, text: str) -> int:
            """Quick heuristic: ~4 chars per token average."""
            return len(text) // 4

        def is_within_limit(self, text: str, limit: int, provider: str = "openai") -> bool:
            """Check if text is within token limit."""
            return self.count_tokens(text, provider) <= limit
    ```

    Include docstrings for all methods.
  </action>
  <verify>
    ```bash
    python -c "from src.ai.completions import TokenCounter; tc = TokenCounter(); print(f'Tokens: {tc.count_tokens(\"Hello world\")}')"
    ```
    Should print token count (around 2-3 tokens)
  </verify>
  <done>TokenCounter counts tokens using tiktoken for OpenAI, fallback for other providers</done>
</task>

<task type="auto">
  <name>Task 3: Create DiffService with models</name>
  <files>
    src/services/diff/__init__.py
    src/services/diff/models.py
    src/services/diff/diff_service.py
  </files>
  <action>
    Create DiffService using Python's difflib:

    1. Create `src/services/diff/__init__.py` with exports
    2. Create `src/services/diff/models.py`:

    ```python
    from dataclasses import dataclass

    @dataclass
    class DiffLine:
        """A single line in a diff."""
        content: str
        line_type: str  # "add", "remove", "context"
        old_line_no: int | None = None
        new_line_no: int | None = None

    @dataclass
    class DiffHunk:
        """A hunk (section) of changes in a diff."""
        source_start: int
        source_length: int
        target_start: int
        target_length: int
        lines: list[DiffLine]
        header: str = ""  # @@ -X,Y +A,B @@ optional context
    ```

    3. Create `src/services/diff/diff_service.py`:

    ```python
    import difflib
    from .models import DiffHunk, DiffLine

    class DiffService:
        """Generate and apply diffs between code versions."""

        def generate_diff(
            self,
            original: str,
            modified: str,
            filename: str = "file",
        ) -> list[DiffHunk]:
            """Generate diff hunks between original and modified.

            Args:
                original: Original content.
                modified: Modified content.
                filename: Filename for diff header.

            Returns:
                List of DiffHunk objects.
            """
            # Implementation using difflib.unified_diff
            # Parse output into DiffHunk/DiffLine objects

        def apply_hunks(self, original: str, hunks: list[DiffHunk]) -> str:
            """Apply selected hunks to original content.

            Args:
                original: Original content.
                hunks: Hunks to apply.

            Returns:
                Modified content with hunks applied.
            """
            # Apply additions, remove deletions

        def apply_single_hunk(self, original: str, hunk: DiffHunk) -> str:
            """Apply a single hunk to original content."""
            return self.apply_hunks(original, [hunk])
    ```

    The generate_diff implementation should:
    - Split strings into lines with splitlines(keepends=True)
    - Use difflib.unified_diff()
    - Parse the unified diff output into DiffHunk/DiffLine objects
    - Handle the @@ header lines to extract line numbers
  </action>
  <verify>
    ```bash
    python -c "
from src.services.diff import DiffService
ds = DiffService()
hunks = ds.generate_diff('hello\nworld', 'hello\nplanet')
print(f'Hunks: {len(hunks)}, Lines: {len(hunks[0].lines) if hunks else 0}')
"
    ```
    Should show at least 1 hunk with multiple lines
  </verify>
  <done>DiffService generates DiffHunk objects from unified diff, can apply hunks back to original</done>
</task>

</tasks>

<verification>
All foundation services importable and functional:

```bash
python -c "
from src.ui.views.code_editor.ai_panel import ChatState, ChatMessage
from src.ai.completions import TokenCounter
from src.services.diff import DiffService, DiffHunk, DiffLine
print('All imports successful')
"
```
</verification>

<success_criteria>
1. ChatState follows listener/notify pattern from EditorState
2. TokenCounter estimates tokens using tiktoken
3. DiffService generates and applies unified diffs
4. All services have docstrings and type hints
5. No circular import issues
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-assisted-editing/04-01-SUMMARY.md`
</output>
