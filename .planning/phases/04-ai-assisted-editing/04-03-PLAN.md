---
phase: 04-ai-assisted-editing
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/ui/views/code_editor/ai_panel/ghost_text.py
  - src/ai/completions/completion_service.py
autonomous: true

must_haves:
  truths:
    - "User sees dimmed suggestion text after typing pause"
    - "User can press Tab to accept suggestion"
    - "User can press Escape to dismiss suggestion"
  artifacts:
    - path: "src/ui/views/code_editor/ai_panel/ghost_text.py"
      provides: "Ghost text overlay component"
      contains: "class GhostTextOverlay"
    - path: "src/ai/completions/completion_service.py"
      provides: "Inline completion service"
      contains: "class CompletionService"
  key_links:
    - from: "ghost_text.py"
      to: "completion_service.py"
      via: "get_completion call"
      pattern: "completion_service"
    - from: "completion_service.py"
      to: "AIGateway"
      via: "generate or chat call"
      pattern: "gateway\\.(generate|chat)"
---

<objective>
Create the ghost text suggestion system: an overlay component that shows dimmed inline suggestions, and a completion service that fetches suggestions from AI providers.

Purpose: Inline suggestions (Copilot-style ghost text) are the most frictionless way for users to get AI help while coding. This is a core differentiating feature.

Output: GhostTextOverlay component and CompletionService ready for CodeEditor integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-ai-assisted-editing/04-CONTEXT.md
@.planning/phases/04-ai-assisted-editing/04-RESEARCH.md
@.planning/phases/04-ai-assisted-editing/04-01-SUMMARY.md

# Services to use
@src/ai/gateway.py
@src/ai/completions/token_counter.py

# Editor integration point
@src/ui/views/code_editor/editor.py (CodeEditor with Stack overlay)
@src/ui/theme.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GhostTextOverlay component</name>
  <files>src/ui/views/code_editor/ai_panel/ghost_text.py</files>
  <action>
    Create GhostTextOverlay component for displaying inline suggestions:

    ```python
    from collections.abc import Callable
    from dataclasses import dataclass
    import flet as ft
    from src.ui.theme import SkynetteTheme

    @dataclass
    class Suggestion:
        """An inline code suggestion."""
        text: str
        position: int  # Character position in editor
        is_multiline: bool = False

    class GhostTextOverlay(ft.Container):
        """Overlay component that displays ghost text suggestions.

        Designed to be layered on top of a TextField in a Stack.
        Shows dimmed suggestion text that appears after the cursor.

        The overlay matches the TextField's font and positioning to
        ensure the ghost text aligns perfectly with typed text.

        Example:
            overlay = GhostTextOverlay(on_accept=handle_accept, on_dismiss=handle_dismiss)
            stack = ft.Stack([text_field, overlay])
        """

        # Ghost text styling (user configurable in future)
        GHOST_COLOR = ft.colors.with_opacity(0.4, ft.colors.WHITE)
        GHOST_FONT = "monospace"
        GHOST_SIZE = 13

        def __init__(
            self,
            on_accept: Callable[[str], None] | None = None,
            on_dismiss: Callable[[], None] | None = None,
        ):
            """Initialize ghost text overlay.

            Args:
                on_accept: Callback when suggestion accepted, receives suggestion text.
                on_dismiss: Callback when suggestion dismissed.
            """
            super().__init__()
            self._on_accept = on_accept
            self._on_dismiss = on_dismiss
            self._current_suggestion: Suggestion | None = None
            self._ghost_text: ft.Text | None = None

            self.visible = False
            self.expand = True

        def build(self) -> None:
            """Build ghost text display."""
            self._ghost_text = ft.Text(
                value="",
                color=self.GHOST_COLOR,
                font_family=self.GHOST_FONT,
                size=self.GHOST_SIZE,
                selectable=False,
            )

            self.content = ft.Container(
                content=self._ghost_text,
                padding=ft.padding.all(8),  # Match TextField padding
            )

        def show_suggestion(self, suggestion: Suggestion, prefix_text: str) -> None:
            """Show a suggestion at the specified position.

            Args:
                suggestion: The suggestion to display.
                prefix_text: Text before cursor (used for positioning).
            """
            self._current_suggestion = suggestion

            # Build display text: prefix (invisible) + suggestion
            # This positions the ghost text after the typed content
            display = prefix_text + suggestion.text

            self._ghost_text.value = display
            self._ghost_text.spans = [
                # Invisible prefix (same as typed text)
                ft.TextSpan(prefix_text, style=ft.TextStyle(color="transparent")),
                # Visible ghost suggestion
                ft.TextSpan(suggestion.text, style=ft.TextStyle(color=self.GHOST_COLOR)),
            ]

            self.visible = True
            self.update()

        def hide_suggestion(self) -> None:
            """Hide current suggestion."""
            self._current_suggestion = None
            self._ghost_text.value = ""
            self._ghost_text.spans = []
            self.visible = False
            self.update()

        def accept(self) -> str | None:
            """Accept current suggestion.

            Returns:
                Accepted suggestion text, or None if no suggestion.
            """
            if self._current_suggestion is None:
                return None

            text = self._current_suggestion.text
            self.hide_suggestion()

            if self._on_accept:
                self._on_accept(text)

            return text

        def dismiss(self) -> None:
            """Dismiss current suggestion without accepting."""
            self.hide_suggestion()
            if self._on_dismiss:
                self._on_dismiss()

        def has_suggestion(self) -> bool:
            """Check if a suggestion is currently displayed."""
            return self._current_suggestion is not None

        @property
        def current_text(self) -> str | None:
            """Get current suggestion text, or None."""
            return self._current_suggestion.text if self._current_suggestion else None
    ```

    Update ai_panel/__init__.py to export GhostTextOverlay and Suggestion.
  </action>
  <verify>
    ```bash
    python -c "
from src.ui.views.code_editor.ai_panel import GhostTextOverlay, Suggestion
s = Suggestion(text=' + 1', position=10)
print(f'Suggestion: {s.text}, multiline: {s.is_multiline}')
"
    ```
  </verify>
  <done>GhostTextOverlay shows dimmed suggestion text, supports accept/dismiss, positions correctly relative to prefix</done>
</task>

<task type="auto">
  <name>Task 2: Create CompletionService for inline suggestions</name>
  <files>src/ai/completions/completion_service.py</files>
  <action>
    Create CompletionService that fetches inline completions from AI:

    ```python
    import asyncio
    from dataclasses import dataclass
    from typing import AsyncIterator

    from src.ai.gateway import AIGateway, AIMessage, GenerationConfig
    from src.ai.completions.token_counter import TokenCounter

    @dataclass
    class CompletionRequest:
        """Request for code completion."""
        code_before: str  # Code before cursor
        code_after: str   # Code after cursor (for context)
        language: str
        max_tokens: int = 50
        provider: str | None = None

    class CompletionService:
        """Service for fetching inline code completions.

        Uses AI Gateway to get completions based on code context.
        Implements debouncing and caching for performance.

        Example:
            service = CompletionService(gateway)
            suggestion = await service.get_completion(request)
        """

        # Debounce delay in seconds
        DEBOUNCE_DELAY = 0.5

        def __init__(self, gateway: AIGateway):
            """Initialize completion service.

            Args:
                gateway: AI Gateway for provider access.
            """
            self.gateway = gateway
            self.token_counter = TokenCounter()
            self._pending_task: asyncio.Task | None = None
            self._cache: dict[str, str] = {}  # Simple cache by code hash

        async def get_completion(self, request: CompletionRequest) -> str | None:
            """Get completion for the given context.

            Args:
                request: Completion request with code context.

            Returns:
                Suggested completion text, or None if no suggestion.
            """
            # Cancel any pending request
            if self._pending_task and not self._pending_task.done():
                self._pending_task.cancel()

            # Check cache
            cache_key = self._make_cache_key(request)
            if cache_key in self._cache:
                return self._cache[cache_key]

            # Debounce
            await asyncio.sleep(self.DEBOUNCE_DELAY)

            # Build prompt for completion
            prompt = self._build_completion_prompt(request)

            # Check token limit
            if not self.token_counter.is_within_limit(prompt, 4000, request.provider or "openai"):
                # Truncate context if too large
                prompt = self._truncate_prompt(prompt, 3000)

            try:
                config = GenerationConfig(
                    max_tokens=request.max_tokens,
                    temperature=0.2,  # Low temp for completions
                    stop_sequences=["\n\n", "```"],  # Stop at natural boundaries
                )

                response = await self.gateway.generate(
                    prompt,
                    config=config,
                    provider=request.provider,
                )

                suggestion = self._clean_suggestion(response.content)

                # Cache result
                self._cache[cache_key] = suggestion

                return suggestion if suggestion else None

            except Exception:
                # Silently fail - inline suggestions are optional
                return None

        def _build_completion_prompt(self, request: CompletionRequest) -> str:
            """Build prompt for completion request."""
            return f"""Complete the following {request.language} code. Only output the completion, no explanation.

```{request.language}
{request.code_before}
```

Continue from where the code ends. Output only the next few tokens/lines that would naturally follow."""

        def _clean_suggestion(self, text: str) -> str:
            """Clean up AI response to extract just the completion."""
            # Remove markdown code blocks if present
            text = text.strip()
            if text.startswith("```"):
                lines = text.split("\n")
                # Remove first and last lines (``` markers)
                if len(lines) > 2:
                    text = "\n".join(lines[1:-1])
                else:
                    text = ""

            # Take only first meaningful chunk
            text = text.strip()
            return text

        def _make_cache_key(self, request: CompletionRequest) -> str:
            """Create cache key from request."""
            # Use last 100 chars of code_before for cache key
            context = request.code_before[-100:] if len(request.code_before) > 100 else request.code_before
            return f"{request.language}:{hash(context)}"

        def _truncate_prompt(self, prompt: str, max_tokens: int) -> str:
            """Truncate prompt to fit within token limit."""
            # Simple character-based truncation (4 chars per token)
            max_chars = max_tokens * 4
            if len(prompt) > max_chars:
                return prompt[-max_chars:]
            return prompt

        def clear_cache(self) -> None:
            """Clear the completion cache."""
            self._cache.clear()

        def cancel_pending(self) -> None:
            """Cancel any pending completion request."""
            if self._pending_task and not self._pending_task.done():
                self._pending_task.cancel()
    ```

    Update completions/__init__.py to export CompletionService and CompletionRequest.
  </action>
  <verify>
    ```bash
    python -c "
from src.ai.completions import CompletionService, CompletionRequest
from src.ai.gateway import AIGateway

gateway = AIGateway()
service = CompletionService(gateway)
req = CompletionRequest(code_before='def hello', code_after='', language='python')
print(f'Request language: {req.language}, max_tokens: {req.max_tokens}')
"
    ```
  </verify>
  <done>CompletionService fetches completions with debouncing, caching, and token limit handling</done>
</task>

</tasks>

<verification>
All components importable:

```bash
python -c "
from src.ui.views.code_editor.ai_panel import GhostTextOverlay, Suggestion
from src.ai.completions import CompletionService, CompletionRequest, TokenCounter
print('Ghost text and completion services ready')
"
```
</verification>

<success_criteria>
1. GhostTextOverlay displays dimmed text aligned with cursor position
2. GhostTextOverlay supports accept() and dismiss() methods
3. CompletionService debounces requests (500ms delay)
4. CompletionService caches recent completions
5. CompletionService handles token limits gracefully
6. Low temperature (0.2) used for deterministic completions
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-assisted-editing/04-03-SUMMARY.md`
</output>
