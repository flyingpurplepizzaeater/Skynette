---
phase: 04-ai-assisted-editing
plan: 06
type: execute
wave: 4
depends_on: ["04-05"]
files_modified:
  - tests/integration/__init__.py
  - tests/integration/test_ai_editing.py
autonomous: true

must_haves:
  truths:
    - "Integration tests verify chat panel sends messages and receives responses"
    - "Integration tests verify diff preview shows changes correctly"
    - "Integration tests verify ghost text suggestions appear and can be accepted"
  artifacts:
    - path: "tests/integration/test_ai_editing.py"
      provides: "Integration tests for AI-assisted editing"
      contains: "test_chat_panel"
      min_lines: 100
  key_links:
    - from: "test_ai_editing.py"
      to: "ChatPanel"
      via: "imports and test methods"
      pattern: "from src.ui.views.code_editor.ai_panel import"
    - from: "test_ai_editing.py"
      to: "DiffPreview"
      via: "imports and test methods"
      pattern: "DiffPreview"
---

<objective>
Create integration tests for the AI-assisted editing flow, covering chat panel interactions, diff preview functionality, and ghost text suggestions.

Purpose: Tests ensure the AI assistance features work correctly and prevent regressions as the codebase evolves. This satisfies QUAL-03.

Output: Integration test suite for AI-assisted editing (tests/integration/test_ai_editing.py).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-ai-assisted-editing/04-05-SUMMARY.md

# Components to test
@src/ui/views/code_editor/ai_panel/chat_state.py
@src/ui/views/code_editor/ai_panel/chat_panel.py
@src/ui/views/code_editor/ai_panel/ghost_text.py
@src/ui/views/code_editor/ai_panel/diff_preview.py
@src/ai/completions/completion_service.py
@src/services/diff/diff_service.py

# Existing test patterns
@tests/ (existing test structure)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration tests for chat and state</name>
  <files>
    tests/integration/__init__.py
    tests/integration/test_ai_editing.py
  </files>
  <action>
    Create integration tests for ChatState, ChatPanel message flow, and provider selection:

    1. **Create tests/integration/__init__.py** if not exists (empty file)

    2. **Create tests/integration/test_ai_editing.py:**
    ```python
    """Integration tests for AI-assisted editing features.

    Tests the interaction between chat panel, diff preview, ghost text,
    and the underlying services.

    Note: These tests don't require a running Flet app - they test
    the component logic and state management directly.
    """

    import asyncio
    import pytest
    from unittest.mock import AsyncMock, MagicMock, patch

    from src.ui.views.code_editor.ai_panel import (
        ChatState,
        ChatMessage,
        ChatPanel,
        DiffPreview,
        GhostTextOverlay,
        Suggestion,
    )
    from src.ai.gateway import AIGateway, AIMessage, AIStreamChunk
    from src.ai.completions import CompletionService, CompletionRequest, TokenCounter
    from src.services.diff import DiffService, DiffHunk, DiffLine


    class TestChatState:
        """Tests for ChatState state management."""

        def test_add_message_notifies_listeners(self):
            """Adding a message should notify all listeners."""
            state = ChatState()
            notified = []
            state.add_listener(lambda: notified.append(True))

            state.add_message("user", "Hello")

            assert len(notified) == 1
            assert len(state.messages) == 1
            assert state.messages[0].role == "user"
            assert state.messages[0].content == "Hello"

        def test_update_last_message_for_streaming(self):
            """update_last_message should modify the last message."""
            state = ChatState()
            state.add_message("assistant", "")

            state.update_last_message("Hello")
            assert state.messages[-1].content == "Hello"

            state.update_last_message("Hello world")
            assert state.messages[-1].content == "Hello world"

        def test_clear_messages(self):
            """clear_messages should remove all messages."""
            state = ChatState()
            state.add_message("user", "Test")
            state.add_message("assistant", "Response")

            state.clear_messages()

            assert len(state.messages) == 0

        def test_set_streaming_notifies(self):
            """set_streaming should update state and notify."""
            state = ChatState()
            notified = []
            state.add_listener(lambda: notified.append(True))

            state.set_streaming(True)

            assert state.is_streaming is True
            assert len(notified) == 1

        def test_remove_listener(self):
            """remove_listener should stop notifications."""
            state = ChatState()
            called = []
            callback = lambda: called.append(True)

            state.add_listener(callback)
            state.add_message("user", "Test")
            assert len(called) == 1

            state.remove_listener(callback)
            state.add_message("user", "Test2")
            assert len(called) == 1  # Not called again

        def test_dispose_clears_state(self):
            """dispose should clear all state and listeners."""
            state = ChatState()
            state.add_listener(lambda: None)
            state.add_message("user", "Test")

            state.dispose()

            assert len(state.messages) == 0
            assert len(state._listeners) == 0


    class TestTokenCounter:
        """Tests for TokenCounter service."""

        def test_count_tokens_openai(self):
            """Should count tokens using OpenAI encoding."""
            counter = TokenCounter()
            count = counter.count_tokens("Hello world", provider="openai")
            assert count > 0
            assert count < 10  # "Hello world" is about 2-3 tokens

        def test_count_tokens_fallback(self):
            """Should use fallback for non-OpenAI providers."""
            counter = TokenCounter()
            count = counter.count_tokens("Hello world", provider="anthropic")
            assert count > 0

        def test_estimate_heuristic(self):
            """Heuristic should estimate ~4 chars per token."""
            counter = TokenCounter()
            # 40 chars = ~10 tokens by heuristic
            estimate = counter.estimate_heuristic("a" * 40)
            assert estimate == 10

        def test_is_within_limit(self):
            """is_within_limit should check against limit."""
            counter = TokenCounter()
            assert counter.is_within_limit("Hello", limit=100)
            # Very long text should exceed small limit
            long_text = "word " * 1000
            assert not counter.is_within_limit(long_text, limit=100)


    class TestDiffService:
        """Tests for DiffService diff generation."""

        def test_generate_diff_single_change(self):
            """Should detect single line change."""
            service = DiffService()
            original = "line1\nline2\nline3"
            modified = "line1\nline2 changed\nline3"

            hunks = service.generate_diff(original, modified)

            assert len(hunks) >= 1
            # Should have both remove and add lines
            line_types = [l.line_type for h in hunks for l in h.lines]
            assert "add" in line_types
            assert "remove" in line_types

        def test_generate_diff_no_changes(self):
            """Should return empty list for identical content."""
            service = DiffService()
            content = "line1\nline2\nline3"

            hunks = service.generate_diff(content, content)

            assert len(hunks) == 0

        def test_generate_diff_addition(self):
            """Should detect line addition."""
            service = DiffService()
            original = "line1\nline2"
            modified = "line1\nline2\nline3"

            hunks = service.generate_diff(original, modified)

            assert len(hunks) >= 1
            add_lines = [l for h in hunks for l in h.lines if l.line_type == "add"]
            assert any("line3" in l.content for l in add_lines)

        def test_apply_hunks(self):
            """Should apply hunks to produce modified content."""
            service = DiffService()
            original = "line1\nline2\nline3"
            modified = "line1\nline2 changed\nline3"

            hunks = service.generate_diff(original, modified)
            result = service.apply_hunks(original, hunks)

            assert result.strip() == modified.strip()


    class TestGhostTextOverlay:
        """Tests for GhostTextOverlay suggestions."""

        def test_show_suggestion(self):
            """Should display suggestion."""
            overlay = GhostTextOverlay()
            suggestion = Suggestion(text=" + 1", position=10)

            overlay.show_suggestion(suggestion, "x = 5")

            assert overlay.has_suggestion()
            assert overlay.current_text == " + 1"

        def test_accept_returns_text(self):
            """accept should return suggestion text."""
            accepted = []
            overlay = GhostTextOverlay(on_accept=lambda t: accepted.append(t))
            overlay.show_suggestion(Suggestion(text="test", position=0), "")

            result = overlay.accept()

            assert result == "test"
            assert accepted == ["test"]
            assert not overlay.has_suggestion()

        def test_dismiss_clears_suggestion(self):
            """dismiss should clear suggestion."""
            dismissed = []
            overlay = GhostTextOverlay(on_dismiss=lambda: dismissed.append(True))
            overlay.show_suggestion(Suggestion(text="test", position=0), "")

            overlay.dismiss()

            assert not overlay.has_suggestion()
            assert len(dismissed) == 1

        def test_accept_without_suggestion_returns_none(self):
            """accept should return None if no suggestion."""
            overlay = GhostTextOverlay()
            result = overlay.accept()
            assert result is None
    ```
  </action>
  <verify>
    ```bash
    cd "C:\Users\karlt\OneDrive\Desktop\Claude\skynette-repo" && python -m pytest tests/integration/test_ai_editing.py -v --tb=short 2>&1 | head -50
    ```
    Tests should pass (or show clear import errors if dependencies not installed)
  </verify>
  <done>Integration tests cover ChatState, TokenCounter, DiffService, and GhostTextOverlay</done>
</task>

<task type="auto">
  <name>Task 2: Add integration tests for completion service and diff preview</name>
  <files>tests/integration/test_ai_editing.py</files>
  <action>
    Add tests for CompletionService and DiffPreview:

    ```python
    # Add to test_ai_editing.py

    class TestCompletionService:
        """Tests for CompletionService with mocked gateway."""

        @pytest.fixture
        def mock_gateway(self):
            """Create a mock AIGateway."""
            gateway = MagicMock(spec=AIGateway)
            response = MagicMock()
            response.content = "completion_text"

            async def mock_generate(*args, **kwargs):
                return response

            gateway.generate = mock_generate
            return gateway

        @pytest.mark.asyncio
        async def test_get_completion_returns_suggestion(self, mock_gateway):
            """Should return completion from gateway."""
            service = CompletionService(mock_gateway)
            service.DEBOUNCE_DELAY = 0  # Skip debounce for test

            request = CompletionRequest(
                code_before="def hello():",
                code_after="",
                language="python",
            )

            result = await service.get_completion(request)

            assert result is not None
            assert "completion" in result or result == "completion_text"

        @pytest.mark.asyncio
        async def test_caches_completions(self, mock_gateway):
            """Should cache completions for same context."""
            service = CompletionService(mock_gateway)
            service.DEBOUNCE_DELAY = 0

            request = CompletionRequest(
                code_before="test",
                code_after="",
                language="python",
            )

            # First call
            await service.get_completion(request)
            # Second call should use cache
            await service.get_completion(request)

            # Gateway should only be called once due to cache
            # Note: Counting calls depends on mock setup

        def test_clear_cache(self, mock_gateway):
            """clear_cache should empty the cache."""
            service = CompletionService(mock_gateway)
            service._cache["test"] = "cached"

            service.clear_cache()

            assert len(service._cache) == 0


    class TestDiffPreview:
        """Tests for DiffPreview component logic."""

        def test_detects_changes(self):
            """Should detect differences between original and modified."""
            original = "line1\nline2"
            modified = "line1\nline2\nline3"

            preview = DiffPreview(original, modified)
            # Build is called lazily in Flet, but we can check state
            preview._diff_service = DiffService()
            hunks = preview._diff_service.generate_diff(original, modified)

            assert len(hunks) > 0

        def test_no_changes_detected(self):
            """Should detect no changes for identical content."""
            content = "line1\nline2"
            preview = DiffPreview(content, content)
            preview._diff_service = DiffService()
            hunks = preview._diff_service.generate_diff(content, content)

            assert len(hunks) == 0

        def test_get_result_without_accepts(self):
            """get_result should return original if nothing accepted."""
            original = "original"
            modified = "modified"
            preview = DiffPreview(original, modified)
            preview._diff_service = DiffService()
            preview._hunks = preview._diff_service.generate_diff(original, modified)

            result = preview.get_result()

            assert result == original

        def test_has_pending_changes(self):
            """has_pending_changes should reflect acceptance state."""
            original = "a\nb"
            modified = "a\nc"
            preview = DiffPreview(original, modified)
            preview._diff_service = DiffService()
            preview._hunks = preview._diff_service.generate_diff(original, modified)
            preview._accepted_hunks = set()

            assert preview.has_pending_changes()

            # Accept all hunks
            preview._accepted_hunks = set(range(len(preview._hunks)))
            assert not preview.has_pending_changes()


    class TestChatMessageFlow:
        """Tests for end-to-end chat message flow."""

        def test_message_with_code_context(self):
            """Messages can have code context attached."""
            state = ChatState()

            state.add_message("user", "Explain this", code_context="x = 5")

            assert state.messages[0].code_context == "x = 5"

        def test_provider_selection_persists(self):
            """Provider selection should persist across messages."""
            state = ChatState()

            state.set_provider("anthropic")
            state.add_message("user", "Hello")

            assert state.selected_provider == "anthropic"

        def test_streaming_flag_toggles(self):
            """is_streaming should toggle correctly."""
            state = ChatState()

            assert not state.is_streaming

            state.set_streaming(True)
            assert state.is_streaming

            state.set_streaming(False)
            assert not state.is_streaming


    # Run with: pytest tests/integration/test_ai_editing.py -v
    ```
  </action>
  <verify>
    ```bash
    cd "C:\Users\karlt\OneDrive\Desktop\Claude\skynette-repo" && python -m pytest tests/integration/test_ai_editing.py -v --tb=short -x 2>&1 | tail -20
    ```
    All tests should pass
  </verify>
  <done>Integration tests cover CompletionService, DiffPreview, and message flow with code context</done>
</task>

</tasks>

<verification>
Full test suite passes:

```bash
cd "C:\Users\karlt\OneDrive\Desktop\Claude\skynette-repo" && python -m pytest tests/integration/test_ai_editing.py -v
```

Expected: All tests pass, covering:
- ChatState state management
- TokenCounter token counting
- DiffService diff generation/application
- GhostTextOverlay suggestion handling
- CompletionService with mocked gateway
- DiffPreview change detection
- Message flow with code context
</verification>

<success_criteria>
1. Tests exist in tests/integration/test_ai_editing.py
2. ChatState tests verify listener/notify pattern
3. TokenCounter tests verify token counting
4. DiffService tests verify diff generation and application
5. GhostTextOverlay tests verify suggestion accept/dismiss
6. CompletionService tests use mocked gateway
7. All tests pass with pytest
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-assisted-editing/04-06-SUMMARY.md`
</output>
