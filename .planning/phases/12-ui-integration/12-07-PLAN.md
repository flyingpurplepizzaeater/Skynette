---
phase: 12-ui-integration
plan: 07
type: execute
wave: 4
depends_on: ["12-01", "12-02", "12-03", "12-04", "12-05", "12-06"]
files_modified:
  - src/ui/app.py
  - src/ui/views/agents.py
  - tests/agent/ui/test_integration_e2e.py
autonomous: false

must_haves:
  truths:
    - "AgentPanel integrates into main app layout"
    - "Agent task can be started from AgentsView"
    - "Panel updates in real-time during execution"
    - "Approval flow works end-to-end"
    - "E2E tests verify full workflow"
  artifacts:
    - path: "src/ui/views/agents.py"
      provides: "AgentsView with AgentPanel integration"
    - path: "tests/agent/ui/test_integration_e2e.py"
      provides: "E2E tests for agent UI"
      min_lines: 100
  key_links:
    - from: "src/ui/app.py"
      to: "AgentPanel"
      via: "layout integration"
      pattern: "AgentPanel"
    - from: "src/ui/views/agents.py"
      to: "AgentExecutor"
      via: "task execution"
      pattern: "AgentExecutor"
---

<objective>
Integrate AgentPanel into main app and create E2E tests for the complete agent UI workflow.

Purpose: This ties all UI components together and satisfies QUAL-05 (performance benchmarks) with integration tests.

Output: Full agent UI integration in the app with comprehensive E2E tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-ui-integration/12-CONTEXT.md
@.planning/phases/12-ui-integration/12-RESEARCH.md

# App structure
@src/ui/app.py
@src/ui/views/agents.py
@src/agent/loop/executor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate AgentPanel into main app layout</name>
  <files>src/ui/app.py, src/ui/views/agents.py</files>
  <action>
**In src/ui/app.py:**

Add AgentPanel to the main layout as a right sidebar:

1. Import AgentPanel:
   ```python
   from src.agent.ui import AgentPanel
   ```

2. In SkynetteApp.__init__:
   - Create self.agent_panel = None (lazy init)
   - Add agent_panel_visible property

3. In build_main_layout() or equivalent:
   - Add toggle button in top navbar for agent panel (robot icon)
   - Create AgentPanel instance when toggled on
   - Layout: Row with [content_area (expand), agent_panel (if visible)]

4. Wire toggle:
   ```python
   def _toggle_agent_panel(self, e):
       if self.agent_panel is None:
           self.agent_panel = AgentPanel(self.page)
       self.agent_panel.toggle_visibility()
       self.page.update()
   ```

**In src/ui/views/agents.py:**

Update AgentsView to connect with AgentPanel:

1. Add task input field and "Run" button if not already present

2. On "Run" click:
   - Create AgentExecutor
   - Start AgentPanel listening: app.agent_panel.start_listening(executor.emitter)
   - Execute task
   - Stop listening on completion

3. Show AgentStatusIndicator in the agents view

4. Wire approval flow:
   - On approval_requested event, show ApprovalSheet
   - Pass approve/reject to ApprovalManager
  </action>
  <verify>Application starts without errors: `python -c "from src.ui.app import SkynetteApp; print('OK')"`</verify>
  <done>AgentPanel appears as right sidebar, toggle button works</done>
</task>

<task type="auto">
  <name>Task 2: Create E2E integration tests</name>
  <files>tests/agent/ui/test_integration_e2e.py</files>
  <action>
Create comprehensive E2E tests for agent UI workflow.

```python
"""
E2E Integration Tests for Agent UI

Tests the full workflow from task input to completion display.
Uses mock executor to avoid real AI calls.
"""

import asyncio
import pytest
from unittest.mock import MagicMock, AsyncMock, patch
from datetime import datetime, timezone

# Test utilities
@pytest.fixture
def mock_page():
    """Create mock Flet page."""
    page = MagicMock()
    page.overlay = []
    page.update = MagicMock()
    return page

@pytest.fixture
def mock_executor():
    """Create mock AgentExecutor with emitter."""
    from src.agent.events.emitter import AgentEventEmitter
    from src.agent.models.event import AgentEvent

    executor = MagicMock()
    executor.emitter = AgentEventEmitter()
    return executor


class TestAgentPanelIntegration:
    """Test AgentPanel with event flow."""

    def test_panel_receives_plan_created_event(self, mock_page, mock_executor):
        """Verify panel updates when plan is created."""
        from src.agent.ui import AgentPanel
        from src.agent.models.event import AgentEvent

        panel = AgentPanel(mock_page)
        panel.start_listening(mock_executor.emitter)

        # Emit plan_created event
        event = AgentEvent(
            type="plan_created",
            data={
                "plan": {
                    "id": "plan-1",
                    "task": "Test task",
                    "overview": "Test overview",
                    "steps": [
                        {"id": "s1", "description": "Step 1", "status": "pending"},
                        {"id": "s2", "description": "Step 2", "status": "pending"},
                    ],
                }
            },
        )

        # Process event
        asyncio.get_event_loop().run_until_complete(
            mock_executor.emitter.emit(event)
        )

        # Verify panel state updated (steps loaded)
        # Note: actual verification depends on panel implementation
        panel.stop_listening()

    def test_panel_updates_on_step_completion(self, mock_page, mock_executor):
        """Verify step status updates on step_completed event."""
        from src.agent.ui import AgentPanel
        from src.agent.models.event import AgentEvent

        panel = AgentPanel(mock_page)

        # Simulate step_completed
        event = AgentEvent(
            type="step_completed",
            data={"step_id": "s1", "result": {"output": "done"}},
        )

        # Panel should handle this without error
        # Detailed verification in unit tests


class TestApprovalFlow:
    """Test approval workflow from event to UI to decision."""

    @pytest.mark.asyncio
    async def test_approval_sheet_shown_on_request(self, mock_page):
        """Verify ApprovalSheet appears when approval requested."""
        from src.agent.ui import ApprovalSheet
        from src.agent.safety.approval import ApprovalRequest
        from src.agent.safety.classification import ActionClassification

        classification = ActionClassification(
            risk_level="destructive",
            reason="File deletion",
            requires_approval=True,
            tool_name="file_delete",
            parameters={"path": "/tmp/test.txt"},
        )

        request = ApprovalRequest(classification=classification)

        sheet = ApprovalSheet(
            request=request,
            on_approve=MagicMock(),
            on_reject=MagicMock(),
        )

        sheet.show(mock_page)
        assert sheet in mock_page.overlay
        assert sheet.open == True

    @pytest.mark.asyncio
    async def test_approval_decision_propagates(self, mock_page):
        """Verify approve/reject calls propagate correctly."""
        from src.agent.ui import ApprovalSheet
        from src.agent.safety.approval import ApprovalRequest
        from src.agent.safety.classification import ActionClassification

        on_approve = MagicMock()
        on_reject = MagicMock()

        classification = ActionClassification(
            risk_level="moderate",
            reason="Test",
            requires_approval=True,
            tool_name="test_tool",
            parameters={},
        )

        request = ApprovalRequest(classification=classification)

        sheet = ApprovalSheet(
            request=request,
            on_approve=on_approve,
            on_reject=on_reject,
        )

        # Simulate approve click
        sheet._handle_approve(False)
        on_approve.assert_called_with(False)


class TestStepViews:
    """Test step view components."""

    def test_checklist_view_renders_steps(self):
        """Verify checklist view shows all steps."""
        from src.agent.ui.step_views import StepChecklistView
        from src.agent.models.plan import PlanStep, StepStatus

        steps = [
            PlanStep(id="1", description="First step", status=StepStatus.COMPLETED),
            PlanStep(id="2", description="Second step", status=StepStatus.RUNNING),
            PlanStep(id="3", description="Third step", status=StepStatus.PENDING),
        ]

        view = StepChecklistView(steps)
        assert len(view.controls) == 3

    def test_step_status_update(self):
        """Verify step status can be updated."""
        from src.agent.ui.step_views import StepChecklistView
        from src.agent.models.plan import PlanStep, StepStatus

        steps = [PlanStep(id="1", description="Test", status=StepStatus.PENDING)]
        view = StepChecklistView(steps)

        view.update_step("1", StepStatus.COMPLETED)
        assert view._steps[0].status == StepStatus.COMPLETED


class TestAuditTrail:
    """Test audit trail view."""

    def test_audit_view_queries_store(self):
        """Verify audit view queries AuditStore."""
        with patch('src.agent.ui.audit_view.get_audit_store') as mock_get:
            mock_store = MagicMock()
            mock_store.query.return_value = []
            mock_get.return_value = mock_store

            from src.agent.ui.audit_view import AuditTrailView

            view = AuditTrailView(session_id="test-session")
            view.refresh()

            mock_store.query.assert_called_once()


class TestTaskHistory:
    """Test task history view."""

    def test_history_loads_sessions(self):
        """Verify history loads from TraceStore."""
        with patch('src.agent.ui.task_history.TraceStore') as MockStore:
            mock_store = MagicMock()
            mock_store.get_sessions.return_value = []
            MockStore.return_value = mock_store

            from src.agent.ui.task_history import TaskHistoryView

            view = TaskHistoryView(on_replay=MagicMock())
            view.refresh()

            mock_store.get_sessions.assert_called()


# Performance benchmarks (QUAL-05)
class TestPerformanceBenchmarks:
    """Performance benchmarks for agent UI."""

    def test_panel_render_time(self, mock_page):
        """Verify panel renders in acceptable time."""
        import time
        from src.agent.ui import AgentPanel

        start = time.perf_counter()
        panel = AgentPanel(mock_page)
        elapsed = time.perf_counter() - start

        # Panel should render in under 100ms
        assert elapsed < 0.1, f"Panel render took {elapsed*1000:.1f}ms"

    def test_step_view_render_many_steps(self):
        """Verify step view handles many steps efficiently."""
        import time
        from src.agent.ui.step_views import StepChecklistView
        from src.agent.models.plan import PlanStep, StepStatus

        # Create 100 steps
        steps = [
            PlanStep(id=str(i), description=f"Step {i}", status=StepStatus.PENDING)
            for i in range(100)
        ]

        start = time.perf_counter()
        view = StepChecklistView(steps)
        elapsed = time.perf_counter() - start

        # Should render 100 steps in under 200ms
        assert elapsed < 0.2, f"100 steps took {elapsed*1000:.1f}ms"

    def test_event_processing_throughput(self, mock_page, mock_executor):
        """Verify event processing can handle rapid events."""
        import time
        from src.agent.ui import AgentPanel
        from src.agent.models.event import AgentEvent

        panel = AgentPanel(mock_page)

        # Emit 50 events rapidly
        events = [
            AgentEvent(type="step_started", data={"step_id": str(i)})
            for i in range(50)
        ]

        start = time.perf_counter()
        for event in events:
            asyncio.get_event_loop().run_until_complete(
                mock_executor.emitter.emit(event)
            )
        elapsed = time.perf_counter() - start

        # 50 events should process in under 500ms
        assert elapsed < 0.5, f"50 events took {elapsed*1000:.1f}ms"
```

Run: `python -m pytest tests/agent/ui/test_integration_e2e.py -v`
  </action>
  <verify>pytest tests/agent/ui/test_integration_e2e.py passes</verify>
  <done>E2E tests pass, covering panel integration, approval flow, and performance</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete agent UI integration with panel, step views, plan views, audit trail, and task history</what-built>
  <how-to-verify>
1. Start the application: `python -m src.main` or equivalent
2. Navigate to Agents view
3. Click the agent panel toggle button (robot icon in navbar)
4. Verify right sidebar appears and can be resized
5. Enter a simple task (e.g., "List files in current directory")
6. Click Run and observe:
   - Plan appears in plan view
   - Steps update as execution progresses
   - Status indicator shows current state
7. If approval needed, verify ApprovalSheet appears
8. After completion, verify audit trail shows actions
9. Check task history shows the completed task
10. Test replay button on task history

Expected:
- Panel resizes smoothly
- Events flow to UI in real-time
- Approval sheet blocks until decision
- All views update correctly
  </how-to-verify>
  <resume-signal>Type "approved" if all UI components work, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
- [ ] AgentPanel toggle button in main navbar
- [ ] Panel renders as right sidebar
- [ ] Task execution updates panel in real-time
- [ ] Approval flow works end-to-end
- [ ] E2E tests pass
- [ ] Performance benchmarks met (QUAL-05)
- [ ] Human verification confirms full workflow
</verification>

<success_criteria>
Agent UI fully integrated into main app. Users can start tasks, view progress, approve actions, and see history. E2E tests verify the complete workflow.
</success_criteria>

<output>
After completion, create `.planning/phases/12-ui-integration/12-07-SUMMARY.md`
</output>
