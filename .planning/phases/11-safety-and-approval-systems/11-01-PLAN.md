---
phase: 11-safety-and-approval-systems
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/agent/safety/__init__.py
  - src/agent/safety/classification.py
  - src/agent/models/tool.py
  - src/agent/__init__.py
autonomous: true

must_haves:
  truths:
    - "Any tool action can be classified into one of four risk levels"
    - "Classification includes human-readable reason"
    - "Built-in tools have predefined risk classifications"
    - "MCP tools default to moderate until first approval"
  artifacts:
    - path: "src/agent/safety/classification.py"
      provides: "RiskLevel type, ActionClassification model, ActionClassifier class"
      exports: ["RiskLevel", "ActionClassification", "ActionClassifier"]
    - path: "src/agent/safety/__init__.py"
      provides: "Package exports"
      contains: "from .classification import"
  key_links:
    - from: "src/agent/safety/classification.py"
      to: "src/agent/models/tool.py"
      via: "ToolDefinition.is_destructive/requires_approval"
      pattern: "ToolDefinition"
---

<objective>
Create the action classification system with four risk levels (safe/moderate/destructive/critical)

Purpose: Foundation for all safety decisions - classification determines approval requirements
Output: RiskLevel type, ActionClassification model, ActionClassifier with tool-specific rules
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-safety-and-approval-systems/11-CONTEXT.md
@.planning/phases/11-safety-and-approval-systems/11-RESEARCH.md
@src/agent/models/tool.py
@src/agent/registry/tool_registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create safety module with classification models</name>
  <files>src/agent/safety/__init__.py, src/agent/safety/classification.py</files>
  <action>
Create the safety module with classification types and models:

1. Create `src/agent/safety/classification.py`:
   - `RiskLevel = Literal["safe", "moderate", "destructive", "critical"]` (use Literal per 07-01 decision)
   - `ActionClassification(BaseModel)` with fields:
     - `risk_level: RiskLevel`
     - `reason: str` - human-readable explanation (e.g., "Destructive: deletes files")
     - `requires_approval: bool`
     - `tool_name: str`
     - `parameters: dict`
     - `timestamp: datetime` with UTC default

2. Create `src/agent/safety/__init__.py`:
   - Export RiskLevel, ActionClassification

Follow existing Pydantic patterns from src/agent/models/tool.py (Field, default_factory).
  </action>
  <verify>Python imports work: `from src.agent.safety import RiskLevel, ActionClassification`</verify>
  <done>RiskLevel and ActionClassification types are defined and importable</done>
</task>

<task type="auto">
  <name>Task 2: Create ActionClassifier with tool-specific rules</name>
  <files>src/agent/safety/classification.py, src/agent/safety/__init__.py</files>
  <action>
Add ActionClassifier class to classification.py:

```python
class ActionClassifier:
    """Classifies tool actions by risk level."""

    # Static classification for built-in tools
    TOOL_CLASSIFICATIONS: dict[str, RiskLevel] = {
        # Safe - read-only operations
        "mock_echo": "safe",
        "web_search": "safe",
        "file_read": "safe",
        "file_list": "safe",
        "rag_query": "safe",

        # Moderate - external interactions, reversible changes
        "browser": "moderate",

        # Destructive - file modifications, code execution
        "file_write": "destructive",
        "code_execute": "destructive",
        "github": "destructive",

        # Critical - file deletion (data loss)
        "file_delete": "critical",
    }

    # Approval requirements by risk level
    APPROVAL_REQUIRED: dict[RiskLevel, bool] = {
        "safe": False,
        "moderate": False,  # May require approval based on autonomy level (Phase 13)
        "destructive": True,
        "critical": True,
    }

    def classify(self, tool_name: str, parameters: dict) -> ActionClassification:
        """Classify an action by tool name and parameters."""
        # Built-in tool classification
        if tool_name in self.TOOL_CLASSIFICATIONS:
            risk = self.TOOL_CLASSIFICATIONS[tool_name]
        else:
            # Unknown/MCP tools default to moderate
            risk = "moderate"

        return ActionClassification(
            risk_level=risk,
            reason=self._get_reason(tool_name, risk, parameters),
            requires_approval=self.APPROVAL_REQUIRED[risk],
            tool_name=tool_name,
            parameters=parameters,
        )

    def _get_reason(self, tool_name: str, risk: RiskLevel, params: dict) -> str:
        """Generate human-readable reason for classification."""
        # Tool-specific reasons with parameter context
        reasons = {
            "file_write": f"Destructive: modifies file {params.get('path', 'unknown')}",
            "file_delete": f"Critical: deletes file {params.get('path', 'unknown')}",
            "code_execute": f"Destructive: executes {params.get('language', 'code')} code",
            "github": f"Destructive: GitHub operation ({params.get('operation', 'unknown')})",
            "browser": f"Moderate: web interaction at {params.get('url', 'unknown URL')}",
        }
        if tool_name in reasons:
            return reasons[tool_name]

        # Generic reasons by risk level
        generic = {
            "safe": f"Safe: read-only operation ({tool_name})",
            "moderate": f"Moderate: external interaction ({tool_name})",
            "destructive": f"Destructive: modifies data ({tool_name})",
            "critical": f"Critical: potential data loss ({tool_name})",
        }
        return generic[risk]
```

Update __init__.py to export ActionClassifier.
  </action>
  <verify>Test classification: `ActionClassifier().classify("file_delete", {"path": "/tmp/test.txt"}).risk_level == "critical"`</verify>
  <done>ActionClassifier returns correct risk levels for built-in tools and defaults to moderate for unknown tools</done>
</task>

<task type="auto">
  <name>Task 3: Export from agent package and add risk colors constant</name>
  <files>src/agent/__init__.py, src/agent/safety/classification.py</files>
  <action>
1. Add RISK_COLORS constant to classification.py (for UI use):
```python
# Risk level colors matching Theme patterns
RISK_COLORS: dict[RiskLevel, str] = {
    "safe": "#22C55E",       # Theme.SUCCESS (green)
    "moderate": "#F59E0B",   # Theme.WARNING (amber)
    "destructive": "#F97316", # Orange
    "critical": "#EF4444",   # Theme.ERROR (red)
}

RISK_LABELS: dict[RiskLevel, str] = {
    "safe": "Safe",
    "moderate": "Moderate",
    "destructive": "Destructive",
    "critical": "Critical",
}
```

2. Update src/agent/__init__.py to export safety module:
   - Add import: `from src.agent.safety import RiskLevel, ActionClassification, ActionClassifier, RISK_COLORS, RISK_LABELS`
   - Add to __all__ list

Ensure imports are alphabetically ordered per codebase convention.
  </action>
  <verify>`from src.agent import RiskLevel, ActionClassifier, RISK_COLORS` imports successfully</verify>
  <done>All classification types and constants exported from src.agent package</done>
</task>

</tasks>

<verification>
```python
# Verification script
from src.agent import RiskLevel, ActionClassification, ActionClassifier, RISK_COLORS, RISK_LABELS

classifier = ActionClassifier()

# Test built-in tools
assert classifier.classify("file_read", {"path": "/tmp/test.txt"}).risk_level == "safe"
assert classifier.classify("browser", {"url": "https://example.com"}).risk_level == "moderate"
assert classifier.classify("file_write", {"path": "/tmp/test.txt"}).risk_level == "destructive"
assert classifier.classify("file_delete", {"path": "/tmp/test.txt"}).risk_level == "critical"

# Test unknown tool defaults to moderate
assert classifier.classify("unknown_mcp_tool", {}).risk_level == "moderate"

# Test reason includes context
result = classifier.classify("file_delete", {"path": "/important.txt"})
assert "/important.txt" in result.reason
assert result.requires_approval == True

# Test colors exist
assert RISK_COLORS["critical"] == "#EF4444"

print("All classification tests passed!")
```
</verification>

<success_criteria>
- RiskLevel Literal type with four values: safe, moderate, destructive, critical
- ActionClassification model with risk_level, reason, requires_approval, tool_name, parameters
- ActionClassifier with TOOL_CLASSIFICATIONS dict for 10 built-in tools
- Unknown tools default to "moderate" risk
- RISK_COLORS and RISK_LABELS constants for UI use
- All types exported from src.agent package
</success_criteria>

<output>
After completion, create `.planning/phases/11-safety-and-approval-systems/11-01-SUMMARY.md`
</output>
