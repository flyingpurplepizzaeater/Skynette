---
phase: 01-stability-audit
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ui/views/ai_hub.py
  - src/ai/models/hub.py
  - tests/unit/test_model_management_audit.py
autonomous: true

must_haves:
  truths:
    - "User can view list of available AI models (local and cloud)"
    - "User can add/configure a new AI provider"
    - "User can switch between configured models"
    - "User can download and manage Ollama models"
    - "Wizard flow completes without errors"
  artifacts:
    - path: "tests/unit/test_model_management_audit.py"
      provides: "Regression tests for Model Management bugs found during audit"
      min_lines: 50
  key_links:
    - from: "src/ui/views/ai_hub.py"
      to: "src/ai/models/hub.py"
      via: "get_hub() and ModelHub methods"
      pattern: "hub\\.(get_|list_|download_)"
---

<objective>
Audit Model Management functionality to identify and fix all bugs.

Purpose: Ensure the AI Hub (model management, provider setup, model downloads) works reliably. This is critical infrastructure for all AI features.
Output: Bug-free Model Management with regression tests capturing all fixes.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-stability-audit/01-RESEARCH.md
@.planning/phases/01-stability-audit/01-CONTEXT.md

# Source files to audit
@src/ui/views/ai_hub.py
@src/ai/models/hub.py
@tests/mocks/ai_providers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Static analysis and manual audit of Model Management</name>
  <files>
    - src/ui/views/ai_hub.py (1669 lines - focus on model management, not refactor)
    - src/ai/models/hub.py
  </files>
  <action>
Run static analysis first:
```bash
ruff check src/ui/views/ai_hub.py src/ai/models/hub.py
mypy src/ui/views/ai_hub.py src/ai/models/hub.py
```

Perform manual code audit of AIHubView focusing on model management features:

1. **Setup Wizard** (wizard_step, selected_providers, provider_configs):
   - Check state persistence across steps (back button preserves selections?)
   - Check validation before advancing steps
   - Check completion flow (what happens after wizard finishes?)

2. **Provider Management** (_build_providers_tab):
   - Check provider add/remove/configure flows
   - Check API key storage security (not logged, encrypted?)
   - Check provider status refresh

3. **Model Library** (_build_model_library_tab):
   - Check model list loading (async, error handling)
   - Check model switching
   - Check Ollama model download/management
   - Check HuggingFace integration

4. **Ollama-specific**:
   - Check ollama_status_icon/text updates
   - Check connection detection
   - Check model pull/delete operations

Document findings with format:
```python
# AUDIT-BUG: [description]
# AUDIT-FIX: [proposed fix]
```

Run existing tests to establish baseline:
```bash
pytest tests/e2e/test_ai_hub*.py tests/unit/test_ai_models.py -v --tb=short
```
  </action>
  <verify>
- `ruff check` and `mypy` complete (note any errors)
- Each of the 4 areas audited and documented
- At least 3 bug findings documented (or explicit "no bugs found" note)
- Existing test results recorded
  </verify>
  <done>
Audit complete with documented findings. Ready to fix bugs and write regression tests.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix bugs and write regression tests</name>
  <files>
    - src/ui/views/ai_hub.py
    - src/ai/models/hub.py
    - tests/unit/test_model_management_audit.py (create)
  </files>
  <action>
For each bug found in Task 1:

1. Write a failing test first:
```python
# tests/unit/test_model_management_audit.py
import pytest
from unittest.mock import AsyncMock, MagicMock

class TestModelManagementAuditFindings:
    """Regression tests for bugs found during Model Management audit."""

    @pytest.fixture
    def mock_page(self):
        page = MagicMock()
        page.update = MagicMock()
        page.run_task = MagicMock()
        return page

    @pytest.mark.xfail(reason="BUG: [description]")
    def test_[bug_description](self, mock_page):
        pass
```

2. Fix the bug in the source file

3. Remove xfail marker, verify test passes

4. Run the test:
```bash
pytest tests/unit/test_model_management_audit.py -v
```

Testing patterns for AIHubView (from research):
- Mock page object (don't render full Flet UI)
- Test view logic separately from rendering
- Use pytest-asyncio for async operations
- Focus on state management correctness (wizard_step, selected_providers)

NOTE: This plan is for FIXING BUGS, not refactoring. The AIHubView refactor is Plan 04. Keep fixes minimal and surgical.

If no bugs found, create characterization tests documenting correct behavior.
  </action>
  <verify>
- `pytest tests/unit/test_model_management_audit.py -v` passes
- All bugs have corresponding regression tests
- No `@pytest.mark.xfail` markers remain
- Run full test suite: `pytest tests/ -v --tb=short` - no new failures
  </verify>
  <done>
All Model Management bugs fixed. Each fix has a regression test. Full test suite passes.
  </done>
</task>

</tasks>

<verification>
```bash
# Static analysis clean
ruff check src/ui/views/ai_hub.py src/ai/models/hub.py
mypy src/ui/views/ai_hub.py src/ai/models/hub.py

# All tests pass
pytest tests/unit/test_model_management_audit.py -v
pytest tests/ -v --tb=short
```
</verification>

<success_criteria>
- Model Management code audited with static analysis and manual review
- All discovered bugs fixed (surgical fixes, not refactor)
- Each bug fix has a regression test
- No existing tests broken
- STAB-02 requirement satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/01-stability-audit/01-02-SUMMARY.md`
</output>
