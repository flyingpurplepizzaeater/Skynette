---
phase: 01-stability-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ui/views/simple_mode.py
  - src/ai/gateway.py
  - tests/unit/test_ai_chat_audit.py
autonomous: true

must_haves:
  truths:
    - "User can send a message in AI Chat and receive a response"
    - "User can start a new conversation"
    - "User can view conversation history"
    - "Errors during AI calls display user-friendly messages"
    - "Streaming responses render progressively without freezing UI"
  artifacts:
    - path: "tests/unit/test_ai_chat_audit.py"
      provides: "Regression tests for AI Chat bugs found during audit"
      min_lines: 50
  key_links:
    - from: "src/ui/views/simple_mode.py"
      to: "src/ai/gateway.py"
      via: "AIGateway.chat() or chat_stream()"
      pattern: "gateway.*chat"
---

<objective>
Audit AI Chat functionality to identify and fix all bugs.

Purpose: Ensure AI Chat (SimpleModeView) works reliably before building new capabilities. This is the primary user-facing chat interface.
Output: Bug-free AI Chat with regression tests capturing all fixes.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-stability-audit/01-RESEARCH.md
@.planning/phases/01-stability-audit/01-CONTEXT.md

# Source files to audit
@src/ui/views/simple_mode.py
@src/ai/gateway.py
@tests/mocks/ai_providers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Static analysis and manual audit of AI Chat</name>
  <files>
    - src/ui/views/simple_mode.py (889 lines)
    - src/ai/gateway.py (306 lines)
  </files>
  <action>
Run static analysis first to identify code issues:
```bash
ruff check src/ui/views/simple_mode.py src/ai/gateway.py
mypy src/ui/views/simple_mode.py src/ai/gateway.py
```

Then perform manual code audit of SimpleModeView (AI Chat UI):
1. Trace message send flow: user input -> gateway call -> response render
2. Check async/sync patterns (Flet 0.80+ is async-first, blocking calls freeze UI)
3. Look for error handling gaps in AI calls
4. Check conversation history management (memory, state persistence)
5. Verify streaming response handling (progressive render, no frozen UI)
6. Check edge cases: empty input, very long messages, rapid sends, network errors

Document findings as inline TODO comments with format:
```python
# AUDIT-BUG: [description of bug]
# AUDIT-FIX: [proposed fix]
```

Also run the existing tests to establish baseline:
```bash
pytest tests/unit/test_ai_*.py tests/e2e/test_ai_*.py -v --tb=short
```

Note any test failures as potential bugs.
  </action>
  <verify>
- `ruff check` and `mypy` complete (note any errors)
- Code has been read and analyzed
- At least 3 bug findings documented (or explicit "no bugs found" note)
- Existing test results recorded
  </verify>
  <done>
Audit complete with documented findings. Ready to fix bugs and write regression tests.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix bugs and write regression tests</name>
  <files>
    - src/ui/views/simple_mode.py
    - src/ai/gateway.py
    - tests/unit/test_ai_chat_audit.py (create)
  </files>
  <action>
For each bug found in Task 1:

1. Write a failing test first (captures the bug):
```python
# tests/unit/test_ai_chat_audit.py
import pytest
from unittest.mock import AsyncMock, MagicMock

class TestAIChatAuditFindings:
    """Regression tests for bugs found during AI Chat audit."""

    @pytest.mark.xfail(reason="BUG: [description]")
    def test_[bug_description](self):
        # Test that would catch this bug if reintroduced
        pass
```

2. Fix the bug in the source file

3. Remove `@pytest.mark.xfail` - test should now pass

4. Run the test to confirm fix:
```bash
pytest tests/unit/test_ai_chat_audit.py -v
```

Use existing mock infrastructure from tests/mocks/ai_providers.py (MockAIProvider, FailingAIProvider, SlowAIProvider).

Key testing patterns (from research):
- Mock page object for unit tests (no full Flet render)
- Use pytest-asyncio for async tests (`@pytest.mark.asyncio`)
- Test error paths explicitly (FailingAIProvider)
- Test with slow responses (SlowAIProvider)

If no bugs found, create a test file with characterization tests documenting current correct behavior (serves as regression suite).
  </action>
  <verify>
- `pytest tests/unit/test_ai_chat_audit.py -v` passes
- All bugs have corresponding regression tests
- No `@pytest.mark.xfail` markers remain (all bugs fixed)
- Run full test suite: `pytest tests/ -v --tb=short` - no new failures introduced
  </verify>
  <done>
All AI Chat bugs fixed. Each fix has a regression test. Full test suite passes.
  </done>
</task>

</tasks>

<verification>
```bash
# Static analysis clean
ruff check src/ui/views/simple_mode.py src/ai/gateway.py
mypy src/ui/views/simple_mode.py src/ai/gateway.py

# All tests pass
pytest tests/unit/test_ai_chat_audit.py -v
pytest tests/ -v --tb=short
```
</verification>

<success_criteria>
- AI Chat code audited with static analysis and manual review
- All discovered bugs fixed
- Each bug fix has a regression test
- No existing tests broken
- STAB-01 requirement satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/01-stability-audit/01-01-SUMMARY.md`
</output>
