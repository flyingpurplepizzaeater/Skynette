---
phase: 07-agent-core-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/agent/registry/__init__.py
  - src/agent/registry/base_tool.py
  - src/agent/registry/tool_registry.py
  - src/agent/models/tool.py
autonomous: true

must_haves:
  truths:
    - "BaseTool defines abstract interface for all agent tools"
    - "ToolRegistry is singleton that manages tool registration and lookup"
    - "ToolDefinition converts to OpenAI and Anthropic formats"
    - "A mock tool can be registered and invoked through the registry"
  artifacts:
    - path: "src/agent/models/tool.py"
      provides: "ToolDefinition, ToolCall, ToolResult models"
      exports: ["ToolDefinition", "ToolCall", "ToolResult"]
    - path: "src/agent/registry/base_tool.py"
      provides: "BaseTool abstract class"
      exports: ["BaseTool"]
    - path: "src/agent/registry/tool_registry.py"
      provides: "ToolRegistry singleton"
      exports: ["ToolRegistry", "get_tool_registry"]
  key_links:
    - from: "src/agent/registry/base_tool.py"
      to: "src/agent/models/tool.py"
      via: "BaseTool.get_definition returns ToolDefinition"
      pattern: "def get_definition.*ToolDefinition"
    - from: "src/agent/registry/tool_registry.py"
      to: "src/agent/registry/base_tool.py"
      via: "Registry stores BaseTool subclasses"
      pattern: "Type\\[BaseTool\\]"
---

<objective>
Create the tool infrastructure: abstract base class, registry singleton, and data models for tool definitions, calls, and results.

Purpose: The tool registry is the extension point for all agent capabilities. It follows the existing NodeRegistry pattern but adapted for LLM tool calling with multi-provider format support.

Output: `src/agent/registry/` with base_tool.py and tool_registry.py, plus `src/agent/models/tool.py`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-agent-core-infrastructure/07-RESEARCH.md

# Pattern to follow
@src/core/nodes/registry.py
@src/core/nodes/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tool data models</name>
  <files>
    src/agent/models/tool.py
  </files>
  <action>
In `src/agent/models/tool.py`:

1. Create `ToolDefinition` Pydantic BaseModel with fields:
   - name: str (unique identifier, e.g., "web_search")
   - description: str (what the tool does, for LLM context)
   - parameters: dict (JSON Schema for parameters)
   - category: str = "general" (for grouping in UI)
   - is_destructive: bool = False (affects safety classification later)
   - requires_approval: bool = False (for Phase 11 safety system)

2. Add methods to ToolDefinition:
   - `to_openai_format() -> dict`: Convert to OpenAI function calling format:
     ```python
     {
         "type": "function",
         "function": {
             "name": self.name,
             "description": self.description,
             "parameters": self.parameters,
         }
     }
     ```
   - `to_anthropic_format() -> dict`: Convert to Anthropic tool format:
     ```python
     {
         "name": self.name,
         "description": self.description,
         "input_schema": self.parameters,
     }
     ```

3. Create `ToolCall` Pydantic BaseModel with fields:
   - id: str = Field(default_factory=lambda: str(uuid4()))
   - tool_name: str
   - parameters: dict = Field(default_factory=dict)
   - created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

4. Create `ToolResult` Pydantic BaseModel with fields:
   - tool_call_id: str (links back to ToolCall.id)
   - success: bool
   - data: Any = None (result data on success)
   - error: Optional[str] = None (error message on failure)
   - duration_ms: float = 0 (execution time)

5. Add method to ToolResult:
   - `@classmethod success_result(cls, tool_call_id: str, data: Any, duration_ms: float = 0) -> "ToolResult"`
   - `@classmethod failure_result(cls, tool_call_id: str, error: str, duration_ms: float = 0) -> "ToolResult"`

Update `src/agent/models/__init__.py` to export ToolDefinition, ToolCall, ToolResult.
  </action>
  <verify>
Run: `python -c "from src.agent.models.tool import ToolDefinition, ToolCall, ToolResult; td = ToolDefinition(name='test', description='A test tool', parameters={'type': 'object'}); print(td.to_openai_format()['function']['name'])"`
Expected: Prints "test"
  </verify>
  <done>ToolDefinition, ToolCall, and ToolResult models exist with format conversion methods</done>
</task>

<task type="auto">
  <name>Task 2: Create BaseTool abstract class</name>
  <files>
    src/agent/registry/__init__.py
    src/agent/registry/base_tool.py
  </files>
  <action>
Create `src/agent/registry/` directory.

In `src/agent/registry/base_tool.py`:

1. Import ABC, abstractmethod from abc, and ToolDefinition, ToolResult from models.

2. Create `AgentContext` Pydantic BaseModel (lightweight context passed to tools):
   - session_id: str
   - variables: dict = Field(default_factory=dict) (shared variables)
   - working_directory: Optional[str] = None (for filesystem tools)

3. Create `BaseTool` abstract class:
   ```python
   class BaseTool(ABC):
       """Abstract base class for all agent tools."""

       # Class attributes - override in subclasses
       name: str = "base_tool"
       description: str = "Base tool description"
       parameters_schema: dict = {"type": "object", "properties": {}}

       @abstractmethod
       async def execute(self, params: dict, context: AgentContext) -> ToolResult:
           """
           Execute the tool with validated parameters.

           Args:
               params: Tool parameters (already validated against schema)
               context: Execution context

           Returns:
               ToolResult with success/failure and data/error
           """
           pass

       def get_definition(self) -> ToolDefinition:
           """Get tool definition for LLM context."""
           return ToolDefinition(
               name=self.name,
               description=self.description,
               parameters=self.parameters_schema,
           )

       def validate_params(self, params: dict) -> tuple[bool, Optional[str]]:
           """
           Validate parameters against schema.
           Returns (is_valid, error_message).
           Basic validation - can be enhanced with jsonschema later.
           """
           # For now, just check required fields from schema
           required = self.parameters_schema.get("required", [])
           for field in required:
               if field not in params:
                   return False, f"Missing required parameter: {field}"
           return True, None
   ```

In `src/agent/registry/__init__.py`:
- Export BaseTool, AgentContext
  </action>
  <verify>
Run: `python -c "from src.agent.registry.base_tool import BaseTool, AgentContext; print(BaseTool.__abstractmethods__)"`
Expected: Prints frozenset containing 'execute'
  </verify>
  <done>BaseTool abstract class exists with execute method, get_definition, and validate_params</done>
</task>

<task type="auto">
  <name>Task 3: Create ToolRegistry singleton with mock tool</name>
  <files>
    src/agent/registry/tool_registry.py
  </files>
  <action>
In `src/agent/registry/tool_registry.py`:

1. Create `ToolRegistry` class following NodeRegistry pattern:
   ```python
   class ToolRegistry:
       """Singleton registry for agent tools."""

       _instance = None
       _tools: dict[str, Type[BaseTool]] = {}

       def __new__(cls):
           if cls._instance is None:
               cls._instance = super().__new__(cls)
               cls._instance._initialized = False
           return cls._instance

       def __init__(self):
           if self._initialized:
               return
           self._initialized = True
           self._tools = {}
           self._load_builtin_tools()

       def _load_builtin_tools(self):
           """Load built-in tools. Extended in later phases."""
           # Register mock tool for testing
           from src.agent.registry.mock_tool import MockTool
           self.register(MockTool)

       def register(self, tool_class: Type[BaseTool]):
           """Register a tool type."""
           if not issubclass(tool_class, BaseTool):
               raise TypeError(f"{tool_class} must be a subclass of BaseTool")
           self._tools[tool_class.name] = tool_class

       def unregister(self, tool_name: str):
           """Unregister a tool."""
           if tool_name in self._tools:
               del self._tools[tool_name]

       def get_tool(self, name: str) -> Optional[BaseTool]:
           """Get tool instance by name."""
           tool_class = self._tools.get(name)
           return tool_class() if tool_class else None

       def get_all_definitions(self) -> list[ToolDefinition]:
           """Get all tool definitions for LLM context."""
           return [cls().get_definition() for cls in self._tools.values()]

       def get_openai_tools(self) -> list[dict]:
           """Get all tools in OpenAI format."""
           return [d.to_openai_format() for d in self.get_all_definitions()]

       def get_anthropic_tools(self) -> list[dict]:
           """Get all tools in Anthropic format."""
           return [d.to_anthropic_format() for d in self.get_all_definitions()]

       @property
       def tool_names(self) -> list[str]:
           """Get all registered tool names."""
           return list(self._tools.keys())
   ```

2. Add module-level factory function:
   ```python
   _registry: Optional[ToolRegistry] = None

   def get_tool_registry() -> ToolRegistry:
       """Get the global tool registry instance."""
       global _registry
       if _registry is None:
           _registry = ToolRegistry()
       return _registry
   ```

3. Create `src/agent/registry/mock_tool.py` with a simple mock tool:
   ```python
   class MockTool(BaseTool):
       """Mock tool for testing the registry and agent loop."""

       name = "mock_echo"
       description = "A mock tool that echoes back the input. For testing only."
       parameters_schema = {
           "type": "object",
           "properties": {
               "message": {
                   "type": "string",
                   "description": "Message to echo back"
               }
           },
           "required": ["message"]
       }

       async def execute(self, params: dict, context: AgentContext) -> ToolResult:
           """Echo the message back."""
           message = params.get("message", "")
           return ToolResult.success_result(
               tool_call_id="mock",
               data={"echo": message, "session_id": context.session_id},
               duration_ms=1.0
           )
   ```

Update `src/agent/registry/__init__.py` to export ToolRegistry, get_tool_registry, BaseTool, AgentContext.
Update `src/agent/__init__.py` to also export registry components.
  </action>
  <verify>
Run: `python -c "
import asyncio
from src.agent.registry import get_tool_registry, AgentContext
registry = get_tool_registry()
print('Tools:', registry.tool_names)
tool = registry.get_tool('mock_echo')
ctx = AgentContext(session_id='test-session')
result = asyncio.run(tool.execute({'message': 'hello'}, ctx))
print('Result:', result.data)
"`
Expected: Prints "Tools: ['mock_echo']" and "Result: {'echo': 'hello', 'session_id': 'test-session'}"
  </verify>
  <done>ToolRegistry singleton exists with register/get/list methods and mock tool for testing</done>
</task>

</tasks>

<verification>
All tool infrastructure importable:
```bash
python -c "from src.agent import ToolDefinition, ToolCall, ToolResult, BaseTool, ToolRegistry, get_tool_registry; print('All imports successful')"
```

Mock tool works end-to-end:
```bash
python -c "
import asyncio
from src.agent.registry import get_tool_registry, AgentContext
from src.agent.models.tool import ToolCall

registry = get_tool_registry()
tool = registry.get_tool('mock_echo')

# Validate params
valid, err = tool.validate_params({'message': 'test'})
print(f'Validation: {valid}')

# Execute
ctx = AgentContext(session_id='verify-session')
result = asyncio.run(tool.execute({'message': 'verification test'}, ctx))
print(f'Execution success: {result.success}')
"
```
</verification>

<success_criteria>
- ToolDefinition has to_openai_format() and to_anthropic_format() methods
- ToolCall and ToolResult are Pydantic models with proper fields
- BaseTool is abstract class with execute(), get_definition(), validate_params()
- ToolRegistry is singleton following NodeRegistry pattern
- MockTool is registered and can be invoked through registry
- get_tool_registry() returns global singleton
</success_criteria>

<output>
After completion, create `.planning/phases/07-agent-core-infrastructure/07-02-SUMMARY.md`
</output>
